{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a64327e3",
   "metadata": {},
   "source": [
    "# Corrective RAG\n",
    "\n",
    "- Corrective-RAG (CRAG) is a strategy for RAG by adding an intelligent evaluation and correction layer to improve the accuracy and reliability of responses, reducing hallucinations."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "9d6f254c",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e0ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "267e2b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.document_loaders import WebBaseLoader\n",
    "from langchain_classic.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02c323d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/rag', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and generationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy pageOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\nConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce weve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or youre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\nPreview\\nIn this guide well build an app that answers questions about the websites content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\nSetup\\nInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\nLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\nComponents\\nWe will need to select three components from LangChains suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFace\\uf8ff Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\uf8ff Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\n\\uf8ff Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\uf8ff Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\n\\uf8ff Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\uf8ff Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\\n    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\\n\\nfrom langchain_openai import AzureOpenAIEmbeddings\\n\\nembeddings = AzureOpenAIEmbeddings(\\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\\n)\\nCopypip install -qU langchain-google-genai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"GOOGLE_API_KEY\"):\\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\\n\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\\nCopypip install -qU langchain-google-vertexai\\nCopyfrom langchain_google_vertexai import VertexAIEmbeddings\\n\\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\\nCopypip install -qU langchain-aws\\nCopyfrom langchain_aws import BedrockEmbeddings\\n\\nembeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\\nCopypip install -qU langchain-huggingface\\nCopyfrom langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\\nCopypip install -qU langchain-ollama\\nCopyfrom langchain_ollama import OllamaEmbeddings\\n\\nembeddings = OllamaEmbeddings(model=\"llama3\")\\nCopypip install -qU langchain-cohere\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"COHERE_API_KEY\"):\\n    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\\n\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\\nCopypip install -qU langchain-mistralai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"MISTRALAI_API_KEY\"):\\n    os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\\n\\nfrom langchain_mistralai import MistralAIEmbeddings\\n\\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\\nCopypip install -qU langchain-nomic\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NOMIC_API_KEY\"):\\n    os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\\n\\nfrom langchain_nomic import NomicEmbeddings\\n\\nembeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\\nCopypip install -qU langchain-nvidia-ai-endpoints\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NVIDIA_API_KEY\"):\\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\\n\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\n\\nembeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\\nCopypip install -qU langchain-voyageai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"VOYAGE_API_KEY\"):\\n    os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\\n\\nfrom langchain-voyageai import VoyageAIEmbeddings\\n\\nembeddings = VoyageAIEmbeddings(model=\"voyage-3\")\\nCopypip install -qU langchain-ibm\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"WATSONX_APIKEY\"):\\n    os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\\n\\nfrom langchain_ibm import WatsonxEmbeddings\\n\\nembeddings = WatsonxEmbeddings(\\n    model_id=\"ibm/slate-125m-english-rtrvr\",\\n    url=\"https://us-south.ml.cloud.ibm.com\",\\n    project_id=\"<WATSONX PROJECT_ID>\",\\n)\\nCopypip install -qU langchain-core\\nCopyfrom langchain_core.embeddings import DeterministicFakeEmbedding\\n\\nembeddings = DeterministicFakeEmbedding(size=4096)\\nCopypip install -qU langchain-isaacus\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"ISAACUS_API_KEY\"):\\nos.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\\n\\nfrom langchain_isaacus import IsaacusEmbeddings\\n\\nembeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\\n\\nSelect a vector store:\\n In-memory Amazon OpenSearch AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\nCopypip install -qU  boto3\\nCopyfrom opensearchpy import RequestsHttpConnection\\n\\nservice = \"es\"  # must set the service as \\'es\\'\\nregion = \"us-east-2\"\\ncredentials = boto3.Session(\\n    aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\\n).get_credentials()\\nawsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\\n\\nvector_store = OpenSearchVectorSearch.from_documents(\\n    docs,\\n    embeddings,\\n    opensearch_url=\"host url\",\\n    http_auth=awsauth,\\n    timeout=300,\\n    use_ssl=True,\\n    verify_certs=True,\\n    connection_class=RequestsHttpConnection,\\n    index_name=\"test-index\",\\n)\\nCopypip install -U \"langchain-astradb\"\\nCopyfrom langchain_astradb import AstraDBVectorStore\\n\\nvector_store = AstraDBVectorStore(\\n    embedding=embeddings,\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    collection_name=\"astra_vector_langchain\",\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n    namespace=ASTRA_DB_NAMESPACE,\\n)\\nCopypip install -qU langchain-chroma\\nCopyfrom langchain_chroma import Chroma\\n\\nvector_store = Chroma(\\n    collection_name=\"example_collection\",\\n    embedding_function=embeddings,\\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\\n)\\nCopypip install -qU langchain-community faiss-cpu\\nCopyimport faiss\\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\\nfrom langchain_community.vectorstores import FAISS\\n\\nembedding_dim = len(embeddings.embed_query(\"hello world\"))\\nindex = faiss.IndexFlatL2(embedding_dim)\\n\\nvector_store = FAISS(\\n    embedding_function=embeddings,\\n    index=index,\\n    docstore=InMemoryDocstore(),\\n    index_to_docstore_id={},\\n)\\nCopypip install -qU langchain-milvus\\nCopyfrom langchain_milvus import Milvus\\n\\nURI = \"./milvus_example.db\"\\n\\nvector_store = Milvus(\\n    embedding_function=embeddings,\\n    connection_args={\"uri\": URI},\\n    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\\n)\\nCopypip install -qU langchain-mongodb\\nCopyfrom langchain_mongodb import MongoDBAtlasVectorSearch\\n\\nvector_store = MongoDBAtlasVectorSearch(\\n    embedding=embeddings,\\n    collection=MONGODB_COLLECTION,\\n    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\\n    relevance_score_fn=\"cosine\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGVector\\n\\nvector_store = PGVector(\\n    embeddings=embeddings,\\n    collection_name=\"my_docs\",\\n    connection=\"postgresql+psycopg://...\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGEngine, PGVectorStore\\n\\npg_engine = PGEngine.from_connection_string(\\n    url=\"postgresql+psycopg://...\"\\n)\\n\\nvector_store = PGVectorStore.create_sync(\\n    engine=pg_engine,\\n    table_name=\\'test_table\\',\\n    embedding_service=embeddings\\n)\\nCopypip install -qU langchain-pinecone\\nCopyfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\n\\npc = Pinecone(api_key=...)\\nindex = pc.Index(index_name)\\n\\nvector_store = PineconeVectorStore(embedding=embeddings, index=index)\\nCopypip install -qU langchain-qdrant\\nCopyfrom qdrant_client.models import Distance, VectorParams\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client import QdrantClient\\n\\nclient = QdrantClient(\":memory:\")\\n\\nvector_size = len(embeddings.embed_query(\"sample text\"))\\n\\nif not client.collection_exists(\"test\"):\\n    client.create_collection(\\n        collection_name=\"test\",\\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\\n    )\\nvector_store = QdrantVectorStore(\\n    client=client,\\n    collection_name=\"test\",\\n    embedding=embeddings,\\n)\\n\\n1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if youre comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and wont fit in a models finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\nLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case well use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class post-content, post-title, or post-header are relevant, so well remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\nSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this well split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\nStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n2. Retrieval and generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow lets write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\nRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding arguments for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLets test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directly for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphs Agentic RAG tutorial for more advanced formulations.\\nRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\n Benefits DrawbacksSearch only when needed  The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls  When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries  By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control  The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed  The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLets try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\nNext steps\\nNow that weve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployment\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNextIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/sql-agent', 'title': 'Build a SQL agent - Docs by LangChain', 'language': 'en'}, page_content='Build a SQL agent - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a SQL agentLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsSetupInstallationLangSmith1. Select an LLM2. Configure the database3. Add tools for database interactions4. Use create_agent5. Run the agent(Optional) Use Studio6. Implement human-in-the-loop reviewNext stepsTutorialsLangChainBuild a SQL agentCopy pageCopy pageOverview\\nIn this tutorial, you will learn how to build an agent that can answer questions about a SQL database using LangChain agents.\\nAt a high level, the agent will:\\n1Fetch the available tables and schemas from the database2Decide which tables are relevant to the question3Fetch the schemas for the relevant tables4Generate a query based on the question and information from the schemas5Double-check the query for common mistakes using an LLM6Execute the query and return the results7Correct mistakes surfaced by the database engine until the query is successful8Formulate a response based on the results\\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agents needs. This will mitigate, though not eliminate, the risks of building a model-driven system.\\nConcepts\\nWe will cover the following concepts:\\n\\nTools for reading from SQL databases\\nLangChain agents\\nHuman-in-the-loop processes\\n\\nSetup\\nInstallation\\npipCopypip install langchain  langgraph  langchain-community\\n\\nLangSmith\\nSet up LangSmith to inspect what is happening inside your chain or agent. Then set the following environment variables:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\n1. Select an LLM\\nSelect a model that supports tool-calling:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFace\\uf8ff Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\uf8ff Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\n\\uf8ff Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\uf8ff Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\n\\uf8ff Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\uf8ff Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\nThe output shown in the examples below used OpenAI.\\n2. Configure the database\\nYou will be creating a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the chinook database, which is a sample database that represents a digital media store.\\nFor convenience, we have hosted the database (Chinook.db) on a public GCS bucket.\\nCopyimport requests, pathlib\\n\\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\\nlocal_path = pathlib.Path(\"Chinook.db\")\\n\\nif local_path.exists():\\n    print(f\"{local_path} already exists, skipping download.\")\\nelse:\\n    response = requests.get(url)\\n    if response.status_code == 200:\\n        local_path.write_bytes(response.content)\\n        print(f\"File downloaded and saved as {local_path}\")\\n    else:\\n        print(f\"Failed to download the file. Status code: {response.status_code}\")\\n\\nWe will use a handy SQL database wrapper available in the langchain_community package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\\nCopyfrom langchain_community.utilities import SQLDatabase\\n\\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\\n\\nprint(f\"Dialect: {db.dialect}\")\\nprint(f\"Available tables: {db.get_usable_table_names()}\")\\nprint(f\\'Sample output: {db.run(\"SELECT * FROM Artist LIMIT 5;\")}\\')\\n\\nCopyDialect: sqlite\\nAvailable tables: [\\'Album\\', \\'Artist\\', \\'Customer\\', \\'Employee\\', \\'Genre\\', \\'Invoice\\', \\'InvoiceLine\\', \\'MediaType\\', \\'Playlist\\', \\'PlaylistTrack\\', \\'Track\\']\\nSample output: [(1, \\'AC/DC\\'), (2, \\'Accept\\'), (3, \\'Aerosmith\\'), (4, \\'Alanis Morissette\\'), (5, \\'Alice In Chains\\')]\\n\\n3. Add tools for database interactions\\nUse the SQLDatabase wrapper available in the langchain_community package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\\nCopyfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\\n\\ntoolkit = SQLDatabaseToolkit(db=db, llm=model)\\n\\ntools = toolkit.get_tools()\\n\\nfor tool in tools:\\n    print(f\"{tool.name}: {tool.description}\\\\n\")\\n\\nCopysql_db_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column \\'xxxx\\' in \\'field list\\', use sql_db_schema to query the correct table fields.\\n\\nsql_db_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\\n\\nsql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.\\n\\nsql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\\n\\n4. Use create_agent\\nUse create_agent to build a ReAct agent with minimal code. The agent will interpret the request and generate a SQL command, which the tools will execute. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful.\\nInitialize the agent with a descriptive system prompt to customize its behavior:\\nCopysystem_prompt = \"\"\"\\nYou are an agent designed to interact with a SQL database.\\nGiven an input question, create a syntactically correct {dialect} query to run,\\nthen look at the results of the query and return the answer. Unless the user\\nspecifies a specific number of examples they wish to obtain, always limit your\\nquery to at most {top_k} results.\\n\\nYou can order the results by a relevant column to return the most interesting\\nexamples in the database. Never query for all the columns from a specific table,\\nonly ask for the relevant columns given the question.\\n\\nYou MUST double check your query before executing it. If you get an error while\\nexecuting a query, rewrite the query and try again.\\n\\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\\ndatabase.\\n\\nTo start you should ALWAYS look at the tables in the database to see what you\\ncan query. Do NOT skip this step.\\n\\nThen you should query the schema of the most relevant tables.\\n\"\"\".format(\\n    dialect=db.dialect,\\n    top_k=5,\\n)\\n\\nNow, create an agent with the model, tools, and prompt:\\nCopyfrom langchain.agents import create_agent\\n\\n\\nagent = create_agent(\\n    model,\\n    tools,\\n    system_prompt=system_prompt,\\n)\\n\\n5. Run the agent\\nRun the agent on a sample query and observe its behavior:\\nCopyquestion = \"Which genre on average has the longest tracks?\"\\n\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhich genre on average has the longest tracks?\\n================================== Ai Message ==================================\\nTool Calls:\\n  sql_db_list_tables (call_BQsWg8P65apHc8BTJ1NPDvnM)\\n Call ID: call_BQsWg8P65apHc8BTJ1NPDvnM\\n  Args:\\n================================= Tool Message =================================\\nName: sql_db_list_tables\\n\\nAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\\n================================== Ai Message ==================================\\nTool Calls:\\n  sql_db_schema (call_i89tjKECFSeERbuACYm4w0cU)\\n Call ID: call_i89tjKECFSeERbuACYm4w0cU\\n  Args:\\n    table_names: Track, Genre\\n================================= Tool Message =================================\\nName: sql_db_schema\\n\\n\\nCREATE TABLE \"Genre\" (\\n\\t\"GenreId\" INTEGER NOT NULL,\\n\\t\"Name\" NVARCHAR(120),\\n\\tPRIMARY KEY (\"GenreId\")\\n)\\n\\n/*\\n3 rows from Genre table:\\nGenreId\\tName\\n1\\tRock\\n2\\tJazz\\n3\\tMetal\\n*/\\n\\n\\nCREATE TABLE \"Track\" (\\n\\t\"TrackId\" INTEGER NOT NULL,\\n\\t\"Name\" NVARCHAR(200) NOT NULL,\\n\\t\"AlbumId\" INTEGER,\\n\\t\"MediaTypeId\" INTEGER NOT NULL,\\n\\t\"GenreId\" INTEGER,\\n\\t\"Composer\" NVARCHAR(220),\\n\\t\"Milliseconds\" INTEGER NOT NULL,\\n\\t\"Bytes\" INTEGER,\\n\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\\n\\tPRIMARY KEY (\"TrackId\"),\\n\\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),\\n\\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),\\n\\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\\n)\\n\\n/*\\n3 rows from Track table:\\nTrackId\\tName\\tAlbumId\\tMediaTypeId\\tGenreId\\tComposer\\tMilliseconds\\tBytes\\tUnitPrice\\n1\\tFor Those About To Rock (We Salute You)\\t1\\t1\\t1\\tAngus Young, Malcolm Young, Brian Johnson\\t343719\\t11170334\\t0.99\\n2\\tBalls to the Wall\\t2\\t2\\t1\\tU. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann\\t342562\\t5510424\\t0.99\\n3\\tFast As a Shark\\t3\\t2\\t1\\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\\t230619\\t3990994\\t0.99\\n*/\\n================================== Ai Message ==================================\\nTool Calls:\\n  sql_db_query_checker (call_G64yYm6R6UauiVPCXJZMA49b)\\n Call ID: call_G64yYm6R6UauiVPCXJZMA49b\\n  Args:\\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\\n================================= Tool Message =================================\\nName: sql_db_query_checker\\n\\nSELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\\n================================== Ai Message ==================================\\nTool Calls:\\n  sql_db_query (call_AnO3SrhD0ODJBxh6dHMwvHwZ)\\n Call ID: call_AnO3SrhD0ODJBxh6dHMwvHwZ\\n  Args:\\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\\n================================= Tool Message =================================\\nName: sql_db_query\\n\\n[(\\'Sci Fi & Fantasy\\', 2911783.0384615385), (\\'Science Fiction\\', 2625549.076923077), (\\'Drama\\', 2575283.78125), (\\'TV Shows\\', 2145041.0215053763), (\\'Comedy\\', 1585263.705882353)]\\n================================== Ai Message ==================================\\n\\nOn average, the genre with the longest tracks is \"Sci Fi & Fantasy\" with an average track length of approximately 2,911,783 milliseconds. This is followed by \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\"\\n\\nThe agent correctly wrote a query, checked the query, and ran it to inform its final response.\\nYou can inspect all aspects of the above run, including steps taken, tools invoked, what prompts were seen by the LLM, and more in the LangSmith trace.\\n(Optional) Use Studio\\nStudio provides a client side loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like Tell me the scheme of the database or Show me the invoices for the 5 top customers. You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.\\nRun your agent in StudioIn addition to the previously mentioned packages, you will need to:Copypip install -U langgraph-cli[inmem]>=0.4.0\\nIn directory you will run in, you will need a langgraph.json file with the following contents:Copy{\\n  \"dependencies\": [\".\"],\\n  \"graphs\": {\\n      \"agent\": \"./sql_agent.py:agent\",\\n      \"graph\": \"./sql_agent_langgraph.py:graph\"\\n  },\\n  \"env\": \".env\"\\n}\\nCreate a file sql_agent.py and insert this:Copy#sql_agent.py for studio\\nimport pathlib\\n\\nfrom langchain.agents import create_agent\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\\nfrom langchain_community.utilities import SQLDatabase\\nimport requests\\n\\n\\n# Initialize an LLM\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\n# Get the database, store it locally\\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\\nlocal_path = pathlib.Path(\"Chinook.db\")\\n\\nif local_path.exists():\\n    print(f\"{local_path} already exists, skipping download.\")\\nelse:\\n    response = requests.get(url)\\n    if response.status_code == 200:\\n        local_path.write_bytes(response.content)\\n        print(f\"File downloaded and saved as {local_path}\")\\n    else:\\n        print(f\"Failed to download the file. Status code: {response.status_code}\")\\n\\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\\n\\n# Create the tools\\ntoolkit = SQLDatabaseToolkit(db=db, llm=model)\\n\\ntools = toolkit.get_tools()\\n\\nfor tool in tools:\\n    print(f\"{tool.name}: {tool.description}\\\\n\")\\n\\n# Use create_agent\\nsystem_prompt = \"\"\"\\nYou are an agent designed to interact with a SQL database.\\nGiven an input question, create a syntactically correct {dialect} query to run,\\nthen look at the results of the query and return the answer. Unless the user\\nspecifies a specific number of examples they wish to obtain, always limit your\\nquery to at most {top_k} results.\\n\\nYou can order the results by a relevant column to return the most interesting\\nexamples in the database. Never query for all the columns from a specific table,\\nonly ask for the relevant columns given the question.\\n\\nYou MUST double check your query before executing it. If you get an error while\\nexecuting a query, rewrite the query and try again.\\n\\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\\ndatabase.\\n\\nTo start you should ALWAYS look at the tables in the database to see what you\\ncan query. Do NOT skip this step.\\n\\nThen you should query the schema of the most relevant tables.\\n\"\"\".format(\\n    dialect=db.dialect,\\n    top_k=5,\\n)\\n\\nagent = create_agent(\\n    model,\\n    tools,\\n    system_prompt=system_prompt,\\n)\\n\\n6. Implement human-in-the-loop review\\nIt can be prudent to check the agents SQL queries before they are executed for any unintended actions or inefficiencies.\\nLangChain agents feature support for built-in human-in-the-loop middleware to add oversight to agent tool calls. Lets configure the agent to pause for human review on calling the sql_db_query tool:\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware \\nfrom langgraph.checkpoint.memory import InMemorySaver \\n\\n\\nagent = create_agent(\\n    model,\\n    tools,\\n    system_prompt=system_prompt,\\n    middleware=[ \\n        HumanInTheLoopMiddleware( \\n            interrupt_on={\"sql_db_query\": True}, \\n            description_prefix=\"Tool execution pending approval\", \\n        ), \\n    ], \\n    checkpointer=InMemorySaver(), \\n)\\n\\nWeve added a checkpointer to our agent to allow execution to be paused and resumed. See the human-in-the-loop guide for detalis on this as well as available middleware configurations.\\nOn running the agent, it will now pause for review before executing the sql_db_query tool:\\nCopyquestion = \"Which genre on average has the longest tracks?\"\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}} \\n\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\\n    config, \\n    stream_mode=\"values\",\\n):\\n    if \"__interrupt__\" in step: \\n        print(\"INTERRUPTED:\") \\n        interrupt = step[\"__interrupt__\"][0] \\n        for request in interrupt.value[\"action_requests\"]: \\n            print(request[\"description\"]) \\n    elif \"messages\" in step:\\n        step[\"messages\"][-1].pretty_print()\\n    else:\\n        pass\\n\\nCopy...\\n\\nINTERRUPTED:\\nTool execution pending approval\\n\\nTool: sql_db_query\\nArgs: {\\'query\\': \\'SELECT g.Name AS Genre, AVG(t.Milliseconds) AS AvgTrackLength FROM Track t JOIN Genre g ON t.GenreId = g.GenreId GROUP BY g.Name ORDER BY AvgTrackLength DESC LIMIT 1;\\'}\\n\\nWe can resume execution, in this case accepting the query, using Command:\\nCopyfrom langgraph.types import Command \\n\\nfor step in agent.stream(\\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}), \\n    config,\\n    stream_mode=\"values\",\\n):\\n    if \"messages\" in step:\\n        step[\"messages\"][-1].pretty_print()\\n    elif \"__interrupt__\" in step:\\n        print(\"INTERRUPTED:\")\\n        interrupt = step[\"__interrupt__\"][0]\\n        for request in interrupt.value[\"action_requests\"]:\\n            print(request[\"description\"])\\n    else:\\n        pass\\n\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  sql_db_query (call_7oz86Epg7lYRqi9rQHbZPS1U)\\n Call ID: call_7oz86Epg7lYRqi9rQHbZPS1U\\n  Args:\\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgDuration FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgDuration DESC LIMIT 5;\\n================================= Tool Message =================================\\nName: sql_db_query\\n\\n[(\\'Sci Fi & Fantasy\\', 2911783.0384615385), (\\'Science Fiction\\', 2625549.076923077), (\\'Drama\\', 2575283.78125), (\\'TV Shows\\', 2145041.0215053763), (\\'Comedy\\', 1585263.705882353)]\\n================================== Ai Message ==================================\\n\\nThe genre with the longest average track length is \"Sci Fi & Fantasy\" with an average duration of about 2,911,783 milliseconds, followed by \"Science Fiction\" and \"Drama.\"\\n\\nRefer to the human-in-the-loop guide for details.\\nNext steps\\nFor deeper customization, check out this tutorial for implementing a SQL agent directly using LangGraph primitives.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a RAG agent with LangChainPreviousBuild a voice agent with LangChainNextIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/supervisor', 'title': 'Build a personal assistant with subagents - Docs by LangChain', 'language': 'en'}, page_content='Build a personal assistant with subagents - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationMulti-agentBuild a personal assistant with subagentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainMulti-agentSubagents: Personal assistantHandoffs: Customer supportRouter: Knowledge baseSkills: SQL assistantLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewWhy use a supervisor?ConceptsSetupInstallationLangSmithComponents1. Define tools2. Create specialized sub-agentsCreate a calendar agentCreate an email agent3. Wrap sub-agents as tools4. Create the supervisor agent5. Use the supervisorExample 1: Simple single-domain requestExample 2: Complex multi-domain requestComplete working exampleUnderstanding the architecture6. Add human-in-the-loop review7. Advanced: Control information flowPass additional conversational context to sub-agentsControl what supervisor receives8. Key takeawaysNext stepsTutorialsMulti-agentBuild a personal assistant with subagentsCopy pageCopy pageOverview\\nThe supervisor pattern is a multi-agent architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow.\\nIn this tutorial, youll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities:\\n\\nA calendar agent that handles scheduling, availability checking, and event management.\\nAn email agent that manages communication, drafts messages, and sends notifications.\\n\\nWe will also incorporate human-in-the-loop review to allow users to approve, edit, and reject actions (such as outbound emails) as desired.\\nWhy use a supervisor?\\nMulti-agent architectures allow you to partition tools across workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements).\\nConcepts\\nWe will cover the following concepts:\\n\\nMulti-agent systems\\nHuman-in-the-loop review\\n\\nSetup\\nInstallation\\nThis tutorial requires the langchain package:\\npipcondaCopypip install langchain\\n\\nFor more details, see our Installation guide.\\nLangSmith\\nSet up LangSmith to inspect what is happening inside your agent. Then set the following environment variables:\\nbashpythonCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nComponents\\nWe will need to select a chat model from LangChains suite of integrations:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFace\\uf8ff Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\uf8ff Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\n\\uf8ff Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\uf8ff Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\n\\uf8ff Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\uf8ff Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\n1. Define tools\\nStart by defining the tools that require structured inputs. In real applications, these would call actual APIs (Google Calendar, SendGrid, etc.). For this tutorial, youll use stubs to demonstrate the pattern.\\nCopyfrom langchain.tools import tool\\n\\n@tool\\ndef create_calendar_event(\\n    title: str,\\n    start_time: str,       # ISO format: \"2024-01-15T14:00:00\"\\n    end_time: str,         # ISO format: \"2024-01-15T15:00:00\"\\n    attendees: list[str],  # email addresses\\n    location: str = \"\"\\n) -> str:\\n    \"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\"\\n    # Stub: In practice, this would call Google Calendar API, Outlook API, etc.\\n    return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} attendees\"\\n\\n\\n@tool\\ndef send_email(\\n    to: list[str],  # email addresses\\n    subject: str,\\n    body: str,\\n    cc: list[str] = []\\n) -> str:\\n    \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\\n    # Stub: In practice, this would call SendGrid, Gmail API, etc.\\n    return f\"Email sent to {\\', \\'.join(to)} - Subject: {subject}\"\\n\\n\\n@tool\\ndef get_available_time_slots(\\n    attendees: list[str],\\n    date: str,  # ISO format: \"2024-01-15\"\\n    duration_minutes: int\\n) -> list[str]:\\n    \"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\\n    # Stub: In practice, this would query calendar APIs\\n    return [\"09:00\", \"14:00\", \"16:00\"]\\n\\n2. Create specialized sub-agents\\nNext, well create specialized sub-agents that handle each domain.\\nCreate a calendar agent\\nThe calendar agent understands natural language scheduling requests and translates them into precise API calls. It handles date parsing, availability checking, and event creation.\\nCopyfrom langchain.agents import create_agent\\n\\n\\nCALENDAR_AGENT_PROMPT = (\\n    \"You are a calendar scheduling assistant. \"\\n    \"Parse natural language scheduling requests (e.g., \\'next Tuesday at 2pm\\') \"\\n    \"into proper ISO datetime formats. \"\\n    \"Use get_available_time_slots to check availability when needed. \"\\n    \"Use create_calendar_event to schedule events. \"\\n    \"Always confirm what was scheduled in your final response.\"\\n)\\n\\ncalendar_agent = create_agent(\\n    model,\\n    tools=[create_calendar_event, get_available_time_slots],\\n    system_prompt=CALENDAR_AGENT_PROMPT,\\n)\\n\\nTest the calendar agent to see how it handles natural language scheduling:\\nCopyquery = \"Schedule a team meeting next Tuesday at 2pm for 1 hour\"\\n\\nfor step in calendar_agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\\n):\\n    for update in step.values():\\n        for message in update.get(\"messages\", []):\\n            message.pretty_print()\\n\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  get_available_time_slots (call_EIeoeIi1hE2VmwZSfHStGmXp)\\n Call ID: call_EIeoeIi1hE2VmwZSfHStGmXp\\n  Args:\\n    attendees: []\\n    date: 2024-06-18\\n    duration_minutes: 60\\n================================= Tool Message =================================\\nName: get_available_time_slots\\n\\n[\"09:00\", \"14:00\", \"16:00\"]\\n================================== Ai Message ==================================\\nTool Calls:\\n  create_calendar_event (call_zgx3iJA66Ut0W8S3NpT93kEB)\\n Call ID: call_zgx3iJA66Ut0W8S3NpT93kEB\\n  Args:\\n    title: Team Meeting\\n    start_time: 2024-06-18T14:00:00\\n    end_time: 2024-06-18T15:00:00\\n    attendees: []\\n================================= Tool Message =================================\\nName: create_calendar_event\\n\\nEvent created: Team Meeting from 2024-06-18T14:00:00 to 2024-06-18T15:00:00 with 0 attendees\\n================================== Ai Message ==================================\\n\\nThe team meeting has been scheduled for next Tuesday, June 18th, at 2:00 PM and will last for 1 hour. If you need to add attendees or a location, please let me know!\\n\\nThe agent parses next Tuesday at 2pm into ISO format (2024-01-16T14:00:00), calculates the end time, calls create_calendar_event, and returns a natural language confirmation.\\nCreate an email agent\\nThe email agent handles message composition and sending. It focuses on extracting recipient information, crafting appropriate subject lines and body text, and managing email communication.\\nCopyEMAIL_AGENT_PROMPT = (\\n    \"You are an email assistant. \"\\n    \"Compose professional emails based on natural language requests. \"\\n    \"Extract recipient information and craft appropriate subject lines and body text. \"\\n    \"Use send_email to send the message. \"\\n    \"Always confirm what was sent in your final response.\"\\n)\\n\\nemail_agent = create_agent(\\n    model,\\n    tools=[send_email],\\n    system_prompt=EMAIL_AGENT_PROMPT,\\n)\\n\\nTest the email agent with a natural language request:\\nCopyquery = \"Send the design team a reminder about reviewing the new mockups\"\\n\\nfor step in email_agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\\n):\\n    for update in step.values():\\n        for message in update.get(\"messages\", []):\\n            message.pretty_print()\\n\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  send_email (call_OMl51FziTVY6CRZvzYfjYOZr)\\n Call ID: call_OMl51FziTVY6CRZvzYfjYOZr\\n  Args:\\n    to: [\\'[email\\xa0protected]\\']\\n    subject: Reminder: Please Review the New Mockups\\n    body: Hi Design Team,\\n\\nThis is a friendly reminder to review the new mockups at your earliest convenience. Your feedback is important to ensure that we stay on track with our project timeline.\\n\\nPlease let me know if you have any questions or need additional information.\\n\\nThank you!\\n\\nBest regards,\\n================================= Tool Message =================================\\nName: send_email\\n\\nEmail sent to [email\\xa0protected] - Subject: Reminder: Please Review the New Mockups\\n================================== Ai Message ==================================\\n\\nI\\'ve sent a reminder to the design team asking them to review the new mockups. If you need any further communication on this topic, just let me know!\\n\\nThe agent infers the recipient from the informal request, crafts a professional subject line and body, calls send_email, and returns a confirmation. Each sub-agent has a narrow focus with domain-specific tools and prompts, allowing it to excel at its specific task.\\n3. Wrap sub-agents as tools\\nNow wrap each sub-agent as a tool that the supervisor can invoke. This is the key architectural step that creates the layered system. The supervisor will see high-level tools like schedule_event, not low-level tools like create_calendar_event.\\nCopy@tool\\ndef schedule_event(request: str) -> str:\\n    \"\"\"Schedule calendar events using natural language.\\n\\n    Use this when the user wants to create, modify, or check calendar appointments.\\n    Handles date/time parsing, availability checking, and event creation.\\n\\n    Input: Natural language scheduling request (e.g., \\'meeting with design team\\n    next Tuesday at 2pm\\')\\n    \"\"\"\\n    result = calendar_agent.invoke({\\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\\n    })\\n    return result[\"messages\"][-1].text\\n\\n\\n@tool\\ndef manage_email(request: str) -> str:\\n    \"\"\"Send emails using natural language.\\n\\n    Use this when the user wants to send notifications, reminders, or any email\\n    communication. Handles recipient extraction, subject generation, and email\\n    composition.\\n\\n    Input: Natural language email request (e.g., \\'send them a reminder about\\n    the meeting\\')\\n    \"\"\"\\n    result = email_agent.invoke({\\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\\n    })\\n    return result[\"messages\"][-1].text\\n\\nThe tool descriptions help the supervisor decide when to use each tool, so make them clear and specific. We return only the sub-agents final response, as the supervisor doesnt need to see intermediate reasoning or tool calls.\\n4. Create the supervisor agent\\nNow create the supervisor that orchestrates the sub-agents. The supervisor only sees high-level tools and makes routing decisions at the domain level, not the individual API level.\\nCopySUPERVISOR_PROMPT = (\\n    \"You are a helpful personal assistant. \"\\n    \"You can schedule calendar events and send emails. \"\\n    \"Break down user requests into appropriate tool calls and coordinate the results. \"\\n    \"When a request involves multiple actions, use multiple tools in sequence.\"\\n)\\n\\nsupervisor_agent = create_agent(\\n    model,\\n    tools=[schedule_event, manage_email],\\n    system_prompt=SUPERVISOR_PROMPT,\\n)\\n\\n5. Use the supervisor\\nNow test your complete system with complex requests that require coordination across multiple domains:\\nExample 1: Simple single-domain request\\nCopyquery = \"Schedule a team standup for tomorrow at 9am\"\\n\\nfor step in supervisor_agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\\n):\\n    for update in step.values():\\n        for message in update.get(\"messages\", []):\\n            message.pretty_print()\\n\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  schedule_event (call_mXFJJDU8bKZadNUZPaag8Lct)\\n Call ID: call_mXFJJDU8bKZadNUZPaag8Lct\\n  Args:\\n    request: Schedule a team standup for tomorrow at 9am with Alice and Bob.\\n================================= Tool Message =================================\\nName: schedule_event\\n\\nThe team standup has been scheduled for tomorrow at 9:00 AM with Alice and Bob. If you need to make any changes or add more details, just let me know!\\n================================== Ai Message ==================================\\n\\nThe team standup with Alice and Bob is scheduled for tomorrow at 9:00 AM. If you need any further arrangements or adjustments, please let me know!\\n\\nThe supervisor identifies this as a calendar task, calls schedule_event, and the calendar agent handles date parsing and event creation.\\nFor full transparency into the information flow, including prompts and responses for each chat model call, check out the LangSmith trace for the above run.\\nExample 2: Complex multi-domain request\\nCopyquery = (\\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\\n    \"and send them an email reminder about reviewing the new mockups.\"\\n)\\n\\nfor step in supervisor_agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\\n):\\n    for update in step.values():\\n        for message in update.get(\"messages\", []):\\n            message.pretty_print()\\n\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  schedule_event (call_YA68mqF0koZItCFPx0kGQfZi)\\n Call ID: call_YA68mqF0koZItCFPx0kGQfZi\\n  Args:\\n    request: meeting with the design team next Tuesday at 2pm for 1 hour\\n  manage_email (call_XxqcJBvVIuKuRK794ZIzlLxx)\\n Call ID: call_XxqcJBvVIuKuRK794ZIzlLxx\\n  Args:\\n    request: send the design team an email reminder about reviewing the new mockups\\n================================= Tool Message =================================\\nName: schedule_event\\n\\nYour meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm. Let me know if you need to add more details or make any changes!\\n================================= Tool Message =================================\\nName: manage_email\\n\\nI\\'ve sent an email reminder to the design team requesting them to review the new mockups. If you need to include more information or recipients, just let me know!\\n================================== Ai Message ==================================\\n\\nYour meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm.\\n\\nI\\'ve also sent an email reminder to the design team, asking them to review the new mockups.\\n\\nLet me know if you\\'d like to add more details to the meeting or include additional information in the email!\\n\\nThe supervisor recognizes this requires both calendar and email actions, calls schedule_event for the meeting, then calls manage_email for the reminder. Each sub-agent completes its task, and the supervisor synthesizes both results into a coherent response.\\nRefer to the LangSmith trace to see the detailed information flow for the above run, including individual chat model prompts and responses.\\nComplete working example\\nHeres everything together in a runnable script:\\nShow View complete codeCopy\"\"\"\\nPersonal Assistant Supervisor Example\\n\\nThis example demonstrates the tool calling pattern for multi-agent systems.\\nA supervisor agent coordinates specialized sub-agents (calendar and email)\\nthat are wrapped as tools.\\n\"\"\"\\n\\nfrom langchain.tools import tool\\nfrom langchain.agents import create_agent\\nfrom langchain.chat_models import init_chat_model\\n\\n# ============================================================================\\n# Step 1: Define low-level API tools (stubbed)\\n# ============================================================================\\n\\n@tool\\ndef create_calendar_event(\\n    title: str,\\n    start_time: str,  # ISO format: \"2024-01-15T14:00:00\"\\n    end_time: str,    # ISO format: \"2024-01-15T15:00:00\"\\n    attendees: list[str],  # email addresses\\n    location: str = \"\"\\n) -> str:\\n    \"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\"\\n    return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} attendees\"\\n\\n\\n@tool\\ndef send_email(\\n    to: list[str],      # email addresses\\n    subject: str,\\n    body: str,\\n    cc: list[str] = []\\n) -> str:\\n    \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\\n    return f\"Email sent to {\\', \\'.join(to)} - Subject: {subject}\"\\n\\n\\n@tool\\ndef get_available_time_slots(\\n    attendees: list[str],\\n    date: str,  # ISO format: \"2024-01-15\"\\n    duration_minutes: int\\n) -> list[str]:\\n    \"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\\n    return [\"09:00\", \"14:00\", \"16:00\"]\\n\\n\\n# ============================================================================\\n# Step 2: Create specialized sub-agents\\n# ============================================================================\\n\\nmodel = init_chat_model(\"claude-haiku-4-5-20251001\")  # for example\\n\\ncalendar_agent = create_agent(\\n    model,\\n    tools=[create_calendar_event, get_available_time_slots],\\n    system_prompt=(\\n        \"You are a calendar scheduling assistant. \"\\n        \"Parse natural language scheduling requests (e.g., \\'next Tuesday at 2pm\\') \"\\n        \"into proper ISO datetime formats. \"\\n        \"Use get_available_time_slots to check availability when needed. \"\\n        \"Use create_calendar_event to schedule events. \"\\n        \"Always confirm what was scheduled in your final response.\"\\n    )\\n)\\n\\nemail_agent = create_agent(\\n    model,\\n    tools=[send_email],\\n    system_prompt=(\\n        \"You are an email assistant. \"\\n        \"Compose professional emails based on natural language requests. \"\\n        \"Extract recipient information and craft appropriate subject lines and body text. \"\\n        \"Use send_email to send the message. \"\\n        \"Always confirm what was sent in your final response.\"\\n    )\\n)\\n\\n# ============================================================================\\n# Step 3: Wrap sub-agents as tools for the supervisor\\n# ============================================================================\\n\\n@tool\\ndef schedule_event(request: str) -> str:\\n    \"\"\"Schedule calendar events using natural language.\\n\\n    Use this when the user wants to create, modify, or check calendar appointments.\\n    Handles date/time parsing, availability checking, and event creation.\\n\\n    Input: Natural language scheduling request (e.g., \\'meeting with design team\\n    next Tuesday at 2pm\\')\\n    \"\"\"\\n    result = calendar_agent.invoke({\\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\\n    })\\n    return result[\"messages\"][-1].text\\n\\n\\n@tool\\ndef manage_email(request: str) -> str:\\n    \"\"\"Send emails using natural language.\\n\\n    Use this when the user wants to send notifications, reminders, or any email\\n    communication. Handles recipient extraction, subject generation, and email\\n    composition.\\n\\n    Input: Natural language email request (e.g., \\'send them a reminder about\\n    the meeting\\')\\n    \"\"\"\\n    result = email_agent.invoke({\\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\\n    })\\n    return result[\"messages\"][-1].text\\n\\n\\n# ============================================================================\\n# Step 4: Create the supervisor agent\\n# ============================================================================\\n\\nsupervisor_agent = create_agent(\\n    model,\\n    tools=[schedule_event, manage_email],\\n    system_prompt=(\\n        \"You are a helpful personal assistant. \"\\n        \"You can schedule calendar events and send emails. \"\\n        \"Break down user requests into appropriate tool calls and coordinate the results. \"\\n        \"When a request involves multiple actions, use multiple tools in sequence.\"\\n    )\\n)\\n\\n# ============================================================================\\n# Step 5: Use the supervisor\\n# ============================================================================\\n\\nif __name__ == \"__main__\":\\n    # Example: User request requiring both calendar and email coordination\\n    user_request = (\\n        \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\\n        \"and send them an email reminder about reviewing the new mockups.\"\\n    )\\n\\n    print(\"User Request:\", user_request)\\n    print(\"\\\\n\" + \"=\"*80 + \"\\\\n\")\\n\\n    for step in supervisor_agent.stream(\\n        {\"messages\": [{\"role\": \"user\", \"content\": user_request}]}\\n    ):\\n        for update in step.values():\\n            for message in update.get(\"messages\", []):\\n                message.pretty_print()\\n\\nUnderstanding the architecture\\nYour system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations. The top layer contains the supervisor that routes to high-level capabilities and synthesizes results.\\nThis separation of concerns provides several benefits: each layer has a focused responsibility, you can add new domains without affecting existing ones, and you can test and iterate on each layer independently.\\n6. Add human-in-the-loop review\\nIt can be prudent to incorporate human-in-the-loop review of sensitive actions. LangChain includes built-in middleware to review tool calls, in this case the tools invoked by sub-agents.\\nLets add human-in-the-loop review to both sub-agents:\\n\\nWe configure the create_calendar_event and send_email tools to interrupt, permitting all response types (approve, edit, reject)\\nWe add a checkpointer only to the top-level agent. This is required to pause and resume execution.\\n\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware \\nfrom langgraph.checkpoint.memory import InMemorySaver \\n\\n\\ncalendar_agent = create_agent(\\n    model,\\n    tools=[create_calendar_event, get_available_time_slots],\\n    system_prompt=CALENDAR_AGENT_PROMPT,\\n    middleware=[ \\n        HumanInTheLoopMiddleware( \\n            interrupt_on={\"create_calendar_event\": True}, \\n            description_prefix=\"Calendar event pending approval\", \\n        ), \\n    ], \\n)\\n\\nemail_agent = create_agent(\\n    model,\\n    tools=[send_email],\\n    system_prompt=EMAIL_AGENT_PROMPT,\\n    middleware=[ \\n        HumanInTheLoopMiddleware( \\n            interrupt_on={\"send_email\": True}, \\n            description_prefix=\"Outbound email pending approval\", \\n        ), \\n    ], \\n)\\n\\nsupervisor_agent = create_agent(\\n    model,\\n    tools=[schedule_event, manage_email],\\n    system_prompt=SUPERVISOR_PROMPT,\\n    checkpointer=InMemorySaver(), \\n)\\n\\nLets repeat the query. Note that we gather interrupt events into a list to access downstream:\\nCopyquery = (\\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\\n    \"and send them an email reminder about reviewing the new mockups.\"\\n)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"6\"}}\\n\\ninterrupts = []\\nfor step in supervisor_agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    config,\\n):\\n    for update in step.values():\\n        if isinstance(update, dict):\\n            for message in update.get(\"messages\", []):\\n                message.pretty_print()\\n        else:\\n            interrupt_ = update[0]\\n            interrupts.append(interrupt_)\\n            print(f\"\\\\nINTERRUPTED: {interrupt_.id}\")\\n\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  schedule_event (call_t4Wyn32ohaShpEZKuzZbl83z)\\n Call ID: call_t4Wyn32ohaShpEZKuzZbl83z\\n  Args:\\n    request: Schedule a meeting with the design team next Tuesday at 2pm for 1 hour.\\n  manage_email (call_JWj4vDJ5VMnvkySymhCBm4IR)\\n Call ID: call_JWj4vDJ5VMnvkySymhCBm4IR\\n  Args:\\n    request: Send an email reminder to the design team about reviewing the new mockups before our meeting next Tuesday at 2pm.\\n\\nINTERRUPTED: 4f994c9721682a292af303ec1a46abb7\\n\\nINTERRUPTED: 2b56f299be313ad8bc689eff02973f16\\n\\nThis time weve interrupted execution. Lets inspect the interrupt events:\\nCopyfor interrupt_ in interrupts:\\n    for request in interrupt_.value[\"action_requests\"]:\\n        print(f\"INTERRUPTED: {interrupt_.id}\")\\n        print(f\"{request[\\'description\\']}\\\\n\")\\n\\nCopyINTERRUPTED: 4f994c9721682a292af303ec1a46abb7\\nCalendar event pending approval\\n\\nTool: create_calendar_event\\nArgs: {\\'title\\': \\'Meeting with the Design Team\\', \\'start_time\\': \\'2024-06-18T14:00:00\\', \\'end_time\\': \\'2024-06-18T15:00:00\\', \\'attendees\\': [\\'design team\\']}\\n\\nINTERRUPTED: 2b56f299be313ad8bc689eff02973f16\\nOutbound email pending approval\\n\\nTool: send_email\\nArgs: {\\'to\\': [\\'[email\\xa0protected]\\'], \\'subject\\': \\'Reminder: Review New Mockups Before Meeting Next Tuesday at 2pm\\', \\'body\\': \"Hello Team,\\\\n\\\\nThis is a reminder to review the new mockups ahead of our meeting scheduled for next Tuesday at 2pm. Your feedback and insights will be valuable for our discussion and next steps.\\\\n\\\\nPlease ensure you\\'ve gone through the designs and are ready to share your thoughts during the meeting.\\\\n\\\\nThank you!\\\\n\\\\nBest regards,\\\\n[Your Name]\"}\\n\\nWe can specify decisions for each interrupt by referring to its ID using a Command. Refer to the human-in-the-loop guide for additional details. For demonstration purposes, here we will accept the calendar event, but edit the subject of the outbound email:\\nCopyfrom langgraph.types import Command \\n\\nresume = {}\\nfor interrupt_ in interrupts:\\n    if interrupt_.id == \"2b56f299be313ad8bc689eff02973f16\":\\n        # Edit email\\n        edited_action = interrupt_.value[\"action_requests\"][0].copy()\\n        edited_action[\"arguments\"][\"subject\"] = \"Mockups reminder\"\\n        resume[interrupt_.id] = {\\n            \"decisions\": [{\"type\": \"edit\", \"edited_action\": edited_action}]\\n        }\\n    else:\\n        resume[interrupt_.id] = {\"decisions\": [{\"type\": \"approve\"}]}\\n\\ninterrupts = []\\nfor step in supervisor_agent.stream(\\n    Command(resume=resume), \\n    config,\\n):\\n    for update in step.values():\\n        if isinstance(update, dict):\\n            for message in update.get(\"messages\", []):\\n                message.pretty_print()\\n        else:\\n            interrupt_ = update[0]\\n            interrupts.append(interrupt_)\\n            print(f\"\\\\nINTERRUPTED: {interrupt_.id}\")\\n\\nCopy================================= Tool Message =================================\\nName: schedule_event\\n\\nYour meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm.\\n================================= Tool Message =================================\\nName: manage_email\\n\\nYour email reminder to the design team has been sent. Heres what was sent:\\n\\n- Recipient: [email\\xa0protected]\\n- Subject: Mockups reminder\\n- Body: A reminder to review the new mockups before the meeting next Tuesday at 2pm, with a request for feedback and readiness for discussion.\\n\\nLet me know if you need any further assistance!\\n================================== Ai Message ==================================\\n\\n- Your meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm.\\n- An email reminder has been sent to the design team about reviewing the new mockups before the meeting.\\n\\nLet me know if you need any further assistance!\\n\\nThe run proceeds with our input.\\n7. Advanced: Control information flow\\nBy default, sub-agents receive only the request string from the supervisor. You might want to pass additional context, such as conversation history or user preferences.\\nPass additional conversational context to sub-agents\\nCopyfrom langchain.tools import tool, ToolRuntime\\n\\n@tool\\ndef schedule_event(\\n    request: str,\\n    runtime: ToolRuntime\\n) -> str:\\n    \"\"\"Schedule calendar events using natural language.\"\"\"\\n    # Customize context received by sub-agent\\n    original_user_message = next(\\n        message for message in runtime.state[\"messages\"]\\n        if message.type == \"human\"\\n    )\\n    prompt = (\\n        \"You are assisting with the following user inquiry:\\\\n\\\\n\"\\n        f\"{original_user_message.text}\\\\n\\\\n\"\\n        \"You are tasked with the following sub-request:\\\\n\\\\n\"\\n        f\"{request}\"\\n    )\\n    result = calendar_agent.invoke({\\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\\n    })\\n    return result[\"messages\"][-1].text\\n\\nThis allows sub-agents to see the full conversation context, which can be useful for resolving ambiguities like schedule it for the same time tomorrow (referencing a previous conversation).\\nYou can see the full context received by the sub agent in the chat model call of the LangSmith trace.\\nControl what supervisor receives\\nYou can also customize what information flows back to the supervisor:\\nCopyimport json\\n\\n@tool\\ndef schedule_event(request: str) -> str:\\n    \"\"\"Schedule calendar events using natural language.\"\"\"\\n    result = calendar_agent.invoke({\\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\\n    })\\n\\n    # Option 1: Return just the confirmation message\\n    return result[\"messages\"][-1].text\\n\\n    # Option 2: Return structured data\\n    # return json.dumps({\\n    #     \"status\": \"success\",\\n    #     \"event_id\": \"evt_123\",\\n    #     \"summary\": result[\"messages\"][-1].text\\n    # })\\n\\nImportant: Make sure sub-agent prompts emphasize that their final message should contain all relevant information. A common failure mode is sub-agents that perform tool calls but dont include the results in their final response.\\n8. Key takeaways\\nThe supervisor pattern creates layers of abstraction where each layer has a clear responsibility. When designing a supervisor system, start with clear domain boundaries and give each sub-agent focused tools and prompts. Write clear tool descriptions for the supervisor, test each layer independently before integration, and control information flow based on your specific needs.\\nWhen to use the supervisor patternUse the supervisor pattern when you have multiple distinct domains (calendar, email, CRM, database), each domain has multiple tools or complex logic, you want centralized workflow control, and sub-agents dont need to converse directly with users.For simpler cases with just a few tools, use a single agent. When agents need to have conversations with users, use handoffs instead. For peer-to-peer collaboration between agents, consider other multi-agent patterns.\\nNext steps\\nLearn about handoffs for agent-to-agent conversations, explore context engineering to fine-tune information flow, read the multi-agent overview to compare different patterns, and use LangSmith to debug and monitor your multi-agent system.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a voice agent with LangChainPreviousBuild customer support with handoffsNextIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/voice-agent', 'title': 'Build a voice agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a voice agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a voice agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewWhat are voice agents?How do voice agents work?1. STT > Agent > TTS architecture (The Sandwich)2. Speech-to-Speech architecture (S2S)Demo Application overviewArchitectureSetup1. Speech-to-textKey conceptsImplementation2. LangChain agentKey conceptsImplementation3. Text-to-speechKey conceptsImplementationPutting it all togetherTutorialsLangChainBuild a voice agent with LangChainCopy pageCopy page\\u200bOverview\\nChat interfaces have dominated how we interact with AI, but recent breakthroughs in multimodal AI are opening up exciting new possibilities. High-quality generative models and expressive text-to-speech (TTS) systems now make it possible to build agents that feel less like tools and more like conversational partners.\\nVoice agents are one example of this. Instead of relying on a keyboard and mouse to type inputs into an agent, you can use spoken words to interact with it. This can be a more natural and engaging way to interact with AI, and can be especially useful for certain contexts.\\n\\u200bWhat are voice agents?\\nVoice agents are agents that can engage in natural spoken conversations with users. These agents combine speech recognition, natural language processing, generative AI, and text-to-speech technologies to create seamless, natural conversations.\\nTheyre suited for a variety of use cases, including:\\n\\nCustomer support\\nPersonal assistants\\nHands-free interfaces\\nCoaching and training\\n\\n\\u200bHow do voice agents work?\\nAt a high level, every voice agent needs to handle three tasks:\\n\\nListen - capture audio and transcribe it\\nThink - interpret intent, reason, plan\\nSpeak - generate audio and stream it back to the user\\n\\nThe difference lies in how these steps are sequenced and coupled. In practice, production agents follow one of two main architectures:\\n\\u200b1. STT > Agent > TTS architecture (The Sandwich)\\nThe Sandwich architecture composes three distinct components: speech-to-text (STT), a text-based LangChain agent, and text-to-speech (TTS).\\n\\nPros:\\n\\nFull control over each component (swap STT/TTS providers as needed)\\nAccess to latest capabilities from modern text-modality models\\nTransparent behavior with clear boundaries between components\\n\\nCons:\\n\\nRequires orchestrating multiple services\\nAdditional complexity in managing the pipeline\\nConversion from speech to text loses information (e.g., tone, emotion)\\n\\n\\u200b2. Speech-to-Speech architecture (S2S)\\nSpeech-to-speech uses a multimodal model that processes audio input and generates audio output natively.\\n\\nPros:\\n\\nSimpler architecture with fewer moving parts\\nTypically lower latency for simple interactions\\nDirect audio processing captures tone and other nuances of speech\\n\\nCons:\\n\\nLimited model options, greater risk of provider lock-in\\nFeatures may lag behind text-modality models\\nLess transparency in how audio is processed\\nReduced controllability and customization options\\n\\nThis guide demonstrates the sandwich architecture to balance performance, controllability, and access to modern model capabilities. The sandwich can achieve sub-700ms latency with some STT and TTS providers while maintaining control over modular components.\\n\\u200bDemo Application overview\\nWell walk through building a voice-based agent using the sandwich architecture. The agent will manage orders for a sandwich shop. The application will demonstrate all three components of the sandwich architecture, using AssemblyAI for STT and Cartesia for TTS (although adapters can be built for most providers).\\nAn end-to-end reference application is available in the voice-sandwich-demo repository. We will walk through that application here.\\nThe demo uses WebSockets for real-time bidirectional communication between the browser and server. The same architecture can be adapted for other transports like telephony systems (Twilio, Vonage) or WebRTC connections.\\n\\u200bArchitecture\\nThe demo implements a streaming pipeline where each stage processes data asynchronously:\\nClient (Browser)\\n\\nCaptures microphone audio and encodes it as PCM\\nEstablishes WebSocket connection to the backend server\\nStreams audio chunks to the server in real-time\\nReceives and plays back synthesized speech audio\\n\\nServer (Python)\\n\\n\\nAccepts WebSocket connections from clients\\n\\n\\nOrchestrates the three-step pipeline:\\n\\nSpeech-to-text (STT): Forwards audio to the STT provider (e.g., AssemblyAI), receives transcript events\\nAgent: Processes transcripts with LangChain agent, streams response tokens\\nText-to-speech (TTS): Sends agent responses to the TTS provider (e.g., Cartesia), receives audio chunks\\n\\n\\n\\nReturns synthesized audio to the client for playback\\n\\n\\nThe pipeline uses async generators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.\\n\\u200bSetup\\nFor detailed installation instructions and setup, see the repository README.\\n\\u200b1. Speech-to-text\\nThe STT stage transforms an incoming audio stream into text transcripts. The implementation uses a producer-consumer pattern to handle audio streaming and transcript reception concurrently.\\n\\u200bKey concepts\\nProducer-Consumer Pattern: Audio chunks are sent to the STT service concurrently with receiving transcript events. This allows transcription to begin before all audio has arrived.\\nEvent Types:\\n\\nstt_chunk: Partial transcripts provided as the STT service processes audio\\nstt_output: Final, formatted transcripts that trigger agent processing\\n\\nWebSocket Connection: Maintains a persistent connection to AssemblyAIs real-time STT API, configured for 16kHz PCM audio with automatic turn formatting.\\n\\u200bImplementation\\nCopyfrom typing import AsyncIterator\\nimport asyncio\\nfrom assemblyai_stt import AssemblyAISTT\\nfrom events import VoiceAgentEvent\\n\\nasync def stt_stream(\\n    audio_stream: AsyncIterator[bytes],\\n) -> AsyncIterator[VoiceAgentEvent]:\\n    \"\"\"\\n    Transform stream: Audio (Bytes)  Voice Events (VoiceAgentEvent)\\n\\n    Uses a producer-consumer pattern where:\\n    - Producer: Reads audio chunks and sends them to AssemblyAI\\n    - Consumer: Receives transcription events from AssemblyAI\\n    \"\"\"\\n    stt = AssemblyAISTT(sample_rate=16000)\\n\\n    async def send_audio():\\n        \"\"\"Background task that pumps audio chunks to AssemblyAI.\"\"\"\\n        try:\\n            async for audio_chunk in audio_stream:\\n                await stt.send_audio(audio_chunk)\\n        finally:\\n            # Signal completion when audio stream ends\\n            await stt.close()\\n\\n    # Launch audio sending in background\\n    send_task = asyncio.create_task(send_audio())\\n\\n    try:\\n        # Receive and yield transcription events as they arrive\\n        async for event in stt.receive_events():\\n            yield event\\n    finally:\\n        # Cleanup\\n        with contextlib.suppress(asyncio.CancelledError):\\n            send_task.cancel()\\n            await send_task\\n        await stt.close()\\n\\nThe application implements an AssemblyAI client to manage the WebSocket connection and message parsing. See below for implementations; similar adapters can be constructed for other STT providers.\\nAssemblyAI ClientCopyclass AssemblyAISTT:\\n    def __init__(self, api_key: str | None = None, sample_rate: int = 16000):\\n        self.api_key = api_key or os.getenv(\"ASSEMBLYAI_API_KEY\")\\n        self.sample_rate = sample_rate\\n        self._ws: WebSocketClientProtocol | None = None\\n\\n    async def send_audio(self, audio_chunk: bytes) -> None:\\n        \"\"\"Send PCM audio bytes to AssemblyAI.\"\"\"\\n        ws = await self._ensure_connection()\\n        await ws.send(audio_chunk)\\n\\n    async def receive_events(self) -> AsyncIterator[STTEvent]:\\n        \"\"\"Yield STT events as they arrive from AssemblyAI.\"\"\"\\n        async for raw_message in self._ws:\\n            message = json.loads(raw_message)\\n\\n            if message[\"type\"] == \"Turn\":\\n                # Final formatted transcript\\n                if message.get(\"turn_is_formatted\"):\\n                    yield STTOutputEvent.create(message[\"transcript\"])\\n                # Partial transcript\\n                else:\\n                    yield STTChunkEvent.create(message[\"transcript\"])\\n\\n    async def _ensure_connection(self) -> WebSocketClientProtocol:\\n        \"\"\"Establish WebSocket connection if not already connected.\"\"\"\\n        if self._ws is None:\\n            url = f\"wss://streaming.assemblyai.com/v3/ws?sample_rate={self.sample_rate}&format_turns=true\"\\n            self._ws = await websockets.connect(\\n                url,\\n                additional_headers={\"Authorization\": self.api_key}\\n            )\\n        return self._ws\\n\\n\\u200b2. LangChain agent\\nThe agent stage processes text transcripts through a LangChain agent and streams the response tokens. In this case, we stream all text content blocks generated by the agent.\\n\\u200bKey concepts\\nStreaming Responses: The agent uses stream_mode=\"messages\" to emit response tokens as theyre generated, rather than waiting for the complete response. This enables the TTS stage to begin synthesis immediately.\\nConversation Memory: A checkpointer maintains conversation state across turns using a unique thread ID. This allows the agent to reference previous exchanges in the conversation.\\n\\u200bImplementation\\nCopyfrom uuid import uuid4\\nfrom langchain.agents import create_agent\\nfrom langchain.messages import HumanMessage\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n\\n# Define agent tools\\ndef add_to_order(item: str, quantity: int) -> str:\\n    \"\"\"Add an item to the customer\\'s sandwich order.\"\"\"\\n    return f\"Added {quantity} x {item} to the order.\"\\n\\ndef confirm_order(order_summary: str) -> str:\\n    \"\"\"Confirm the final order with the customer.\"\"\"\\n    return f\"Order confirmed: {order_summary}. Sending to kitchen.\"\\n\\n# Create agent with tools and memory\\nagent = create_agent(\\n    model=\"anthropic:claude-haiku-4-5\",  # Select your model\\n    tools=[add_to_order, confirm_order],\\n    system_prompt=\"\"\"You are a helpful sandwich shop assistant.\\n    Your goal is to take the user\\'s order. Be concise and friendly.\\n    Do NOT use emojis, special characters, or markdown.\\n    Your responses will be read by a text-to-speech engine.\"\"\",\\n    checkpointer=InMemorySaver(),\\n)\\n\\nasync def agent_stream(\\n    event_stream: AsyncIterator[VoiceAgentEvent],\\n) -> AsyncIterator[VoiceAgentEvent]:\\n    \"\"\"\\n    Transform stream: Voice Events  Voice Events (with Agent Responses)\\n\\n    Passes through all upstream events and adds agent_chunk events\\n    when processing STT transcripts.\\n    \"\"\"\\n    # Generate unique thread ID for conversation memory\\n    thread_id = str(uuid4())\\n\\n    async for event in event_stream:\\n        # Pass through all upstream events\\n        yield event\\n\\n        # Process final transcripts through the agent\\n        if event.type == \"stt_output\":\\n            # Stream agent response with conversation context\\n            stream = agent.astream(\\n                {\"messages\": [HumanMessage(content=event.transcript)]},\\n                {\"configurable\": {\"thread_id\": thread_id}},\\n                stream_mode=\"messages\",\\n            )\\n\\n            # Yield agent response chunks as they arrive\\n            async for message, _ in stream:\\n                if message.text:\\n                    yield AgentChunkEvent.create(message.text)\\n\\n\\u200b3. Text-to-speech\\nThe TTS stage synthesizes agent response text into audio and streams it back to the client. Like the STT stage, it uses a producer-consumer pattern to handle concurrent text sending and audio reception.\\n\\u200bKey concepts\\nConcurrent Processing: The implementation merges two async streams:\\n\\nUpstream processing: Passes through all events and sends agent text chunks to the TTS provider\\nAudio reception: Receives synthesized audio chunks from the TTS provider\\n\\nStreaming TTS: Some providers (such as Cartesia) begin synthesizing audio as soon as it receives text, enabling audio playback to start before the agent finishes generating its complete response.\\nEvent Passthrough: All upstream events flow through unchanged, allowing the client or other observers to track the full pipeline state.\\n\\u200bImplementation\\nCopyfrom cartesia_tts import CartesiaTTS\\nfrom utils import merge_async_iters\\n\\nasync def tts_stream(\\n    event_stream: AsyncIterator[VoiceAgentEvent],\\n) -> AsyncIterator[VoiceAgentEvent]:\\n    \"\"\"\\n    Transform stream: Voice Events  Voice Events (with Audio)\\n\\n    Merges two concurrent streams:\\n    1. process_upstream(): passes through events and sends text to Cartesia\\n    2. tts.receive_events(): yields audio chunks from Cartesia\\n    \"\"\"\\n    tts = CartesiaTTS()\\n\\n    async def process_upstream() -> AsyncIterator[VoiceAgentEvent]:\\n        \"\"\"Process upstream events and send agent text to Cartesia.\"\"\"\\n        async for event in event_stream:\\n            # Pass through all events\\n            yield event\\n            # Send agent text to Cartesia for synthesis\\n            if event.type == \"agent_chunk\":\\n                await tts.send_text(event.text)\\n\\n    try:\\n        # Merge upstream events with TTS audio events\\n        # Both streams run concurrently\\n        async for event in merge_async_iters(\\n            process_upstream(),\\n            tts.receive_events()\\n        ):\\n            yield event\\n    finally:\\n        await tts.close()\\n\\nThe application implements an Cartesia client to manage the WebSocket connection and audio streaming. See below for implementations; similar adapters can be constructed for other TTS providers.\\nCartesia ClientCopyimport base64\\nimport json\\nimport websockets\\n\\nclass CartesiaTTS:\\n    def __init__(\\n        self,\\n        api_key: Optional[str] = None,\\n        voice_id: str = \"f6ff7c0c-e396-40a9-a70b-f7607edb6937\",\\n        model_id: str = \"sonic-3\",\\n        sample_rate: int = 24000,\\n        encoding: str = \"pcm_s16le\",\\n    ):\\n        self.api_key = api_key or os.getenv(\"CARTESIA_API_KEY\")\\n        self.voice_id = voice_id\\n        self.model_id = model_id\\n        self.sample_rate = sample_rate\\n        self.encoding = encoding\\n        self._ws: WebSocketClientProtocol | None = None\\n\\n    def _generate_context_id(self) -> str:\\n        \"\"\"Generate a valid context_id for Cartesia.\"\"\"\\n        timestamp = int(time.time() * 1000)\\n        counter = self._context_counter\\n        self._context_counter += 1\\n        return f\"ctx_{timestamp}_{counter}\"\\n\\n    async def send_text(self, text: str | None) -> None:\\n        \"\"\"Send text to Cartesia for synthesis.\"\"\"\\n        if not text or not text.strip():\\n            return\\n\\n        ws = await self._ensure_connection()\\n        payload = {\\n            \"model_id\": self.model_id,\\n            \"transcript\": text,\\n            \"voice\": {\\n                \"mode\": \"id\",\\n                \"id\": self.voice_id,\\n            },\\n            \"output_format\": {\\n                \"container\": \"raw\",\\n                \"encoding\": self.encoding,\\n                \"sample_rate\": self.sample_rate,\\n            },\\n            \"language\": self.language,\\n            \"context_id\": self._generate_context_id(),\\n        }\\n        await ws.send(json.dumps(payload))\\n\\n    async def receive_events(self) -> AsyncIterator[TTSChunkEvent]:\\n        \"\"\"Yield audio chunks as they arrive from Cartesia.\"\"\"\\n        async for raw_message in self._ws:\\n            message = json.loads(raw_message)\\n\\n            # Decode and yield audio chunks\\n            if \"data\" in message and message[\"data\"]:\\n                audio_chunk = base64.b64decode(message[\"data\"])\\n                if audio_chunk:\\n                    yield TTSChunkEvent.create(audio_chunk)\\n\\n    async def _ensure_connection(self) -> WebSocketClientProtocol:\\n        \"\"\"Establish WebSocket connection if not already connected.\"\"\"\\n        if self._ws is None:\\n            url = (\\n                f\"wss://api.cartesia.ai/tts/websocket\"\\n                f\"?api_key={self.api_key}&cartesia_version={self.cartesia_version}\"\\n            )\\n            self._ws = await websockets.connect(url)\\n\\n        return self._ws\\n\\n\\u200bPutting it all together\\nThe complete pipeline chains the three stages together:\\nCopyfrom langchain_core.runnables import RunnableGenerator\\n\\npipeline = (\\n    RunnableGenerator(stt_stream)      # Audio  STT events\\n    | RunnableGenerator(agent_stream)  # STT events  Agent events\\n    | RunnableGenerator(tts_stream)    # Agent events  TTS audio\\n)\\n\\n# Use in WebSocket endpoint\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n    await websocket.accept()\\n\\n    async def websocket_audio_stream():\\n        \"\"\"Yield audio bytes from WebSocket.\"\"\"\\n        while True:\\n            data = await websocket.receive_bytes()\\n            yield data\\n\\n    # Transform audio through pipeline\\n    output_stream = pipeline.atransform(websocket_audio_stream())\\n\\n    # Send TTS audio back to client\\n    async for event in output_stream:\\n        if event.type == \"tts_chunk\":\\n            await websocket.send_bytes(event.audio)\\n\\nWe use RunnableGenerators to compose each step of the pipeline. This is an abstraction LangChain uses internally to manage streaming across components.\\nEach stage processes events independently and concurrently: audio transcription begins as soon as audio arrives, the agent starts reasoning as soon as a transcript is available, and speech synthesis begins as soon as agent text is generated. This architecture can achieve sub-700ms latency to support natural conversation.\\nFor more on building agents with LangChain, see the Agents guide.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a SQL agentPreviousBuild a personal assistant with subagentsNextIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=[\n",
    "    \"https://docs.langchain.com/oss/python/langchain/rag\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/sql-agent\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/supervisor\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/voice-agent\"\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in urls]\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f1167e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/rag', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and generationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy pageOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\nConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce weve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or youre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\nPreview\\nIn this guide well build an app that answers questions about the websites content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\nSetup\\nInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\nLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\nComponents\\nWe will need to select three components from LangChains suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFace\\uf8ff Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\uf8ff Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\n\\uf8ff Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\uf8ff Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\n\\uf8ff Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\uf8ff Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\\n    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\\n\\nfrom langchain_openai import AzureOpenAIEmbeddings\\n\\nembeddings = AzureOpenAIEmbeddings(\\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\\n)\\nCopypip install -qU langchain-google-genai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"GOOGLE_API_KEY\"):\\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\\n\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\\nCopypip install -qU langchain-google-vertexai\\nCopyfrom langchain_google_vertexai import VertexAIEmbeddings\\n\\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\\nCopypip install -qU langchain-aws\\nCopyfrom langchain_aws import BedrockEmbeddings\\n\\nembeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\\nCopypip install -qU langchain-huggingface\\nCopyfrom langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\\nCopypip install -qU langchain-ollama\\nCopyfrom langchain_ollama import OllamaEmbeddings\\n\\nembeddings = OllamaEmbeddings(model=\"llama3\")\\nCopypip install -qU langchain-cohere\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"COHERE_API_KEY\"):\\n    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\\n\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\\nCopypip install -qU langchain-mistralai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"MISTRALAI_API_KEY\"):\\n    os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\\n\\nfrom langchain_mistralai import MistralAIEmbeddings\\n\\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\\nCopypip install -qU langchain-nomic\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NOMIC_API_KEY\"):\\n    os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\\n\\nfrom langchain_nomic import NomicEmbeddings\\n\\nembeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\\nCopypip install -qU langchain-nvidia-ai-endpoints\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NVIDIA_API_KEY\"):\\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\\n\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\n\\nembeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\\nCopypip install -qU langchain-voyageai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"VOYAGE_API_KEY\"):\\n    os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\\n\\nfrom langchain-voyageai import VoyageAIEmbeddings\\n\\nembeddings = VoyageAIEmbeddings(model=\"voyage-3\")\\nCopypip install -qU langchain-ibm\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"WATSONX_APIKEY\"):\\n    os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\\n\\nfrom langchain_ibm import WatsonxEmbeddings\\n\\nembeddings = WatsonxEmbeddings(\\n    model_id=\"ibm/slate-125m-english-rtrvr\",\\n    url=\"https://us-south.ml.cloud.ibm.com\",\\n    project_id=\"<WATSONX PROJECT_ID>\",\\n)\\nCopypip install -qU langchain-core\\nCopyfrom langchain_core.embeddings import DeterministicFakeEmbedding\\n\\nembeddings = DeterministicFakeEmbedding(size=4096)\\nCopypip install -qU langchain-isaacus\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"ISAACUS_API_KEY\"):\\nos.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\\n\\nfrom langchain_isaacus import IsaacusEmbeddings\\n\\nembeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\\n\\nSelect a vector store:\\n In-memory Amazon OpenSearch AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\nCopypip install -qU  boto3\\nCopyfrom opensearchpy import RequestsHttpConnection\\n\\nservice = \"es\"  # must set the service as \\'es\\'\\nregion = \"us-east-2\"\\ncredentials = boto3.Session(\\n    aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\\n).get_credentials()\\nawsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\\n\\nvector_store = OpenSearchVectorSearch.from_documents(\\n    docs,\\n    embeddings,\\n    opensearch_url=\"host url\",\\n    http_auth=awsauth,\\n    timeout=300,\\n    use_ssl=True,\\n    verify_certs=True,\\n    connection_class=RequestsHttpConnection,\\n    index_name=\"test-index\",\\n)\\nCopypip install -U \"langchain-astradb\"\\nCopyfrom langchain_astradb import AstraDBVectorStore\\n\\nvector_store = AstraDBVectorStore(\\n    embedding=embeddings,\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    collection_name=\"astra_vector_langchain\",\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n    namespace=ASTRA_DB_NAMESPACE,\\n)\\nCopypip install -qU langchain-chroma\\nCopyfrom langchain_chroma import Chroma\\n\\nvector_store = Chroma(\\n    collection_name=\"example_collection\",\\n    embedding_function=embeddings,\\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\\n)\\nCopypip install -qU langchain-community faiss-cpu\\nCopyimport faiss\\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\\nfrom langchain_community.vectorstores import FAISS\\n\\nembedding_dim = len(embeddings.embed_query(\"hello world\"))\\nindex = faiss.IndexFlatL2(embedding_dim)\\n\\nvector_store = FAISS(\\n    embedding_function=embeddings,\\n    index=index,\\n    docstore=InMemoryDocstore(),\\n    index_to_docstore_id={},\\n)\\nCopypip install -qU langchain-milvus\\nCopyfrom langchain_milvus import Milvus\\n\\nURI = \"./milvus_example.db\"\\n\\nvector_store = Milvus(\\n    embedding_function=embeddings,\\n    connection_args={\"uri\": URI},\\n    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\\n)\\nCopypip install -qU langchain-mongodb\\nCopyfrom langchain_mongodb import MongoDBAtlasVectorSearch\\n\\nvector_store = MongoDBAtlasVectorSearch(\\n    embedding=embeddings,\\n    collection=MONGODB_COLLECTION,\\n    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\\n    relevance_score_fn=\"cosine\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGVector\\n\\nvector_store = PGVector(\\n    embeddings=embeddings,\\n    collection_name=\"my_docs\",\\n    connection=\"postgresql+psycopg://...\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGEngine, PGVectorStore\\n\\npg_engine = PGEngine.from_connection_string(\\n    url=\"postgresql+psycopg://...\"\\n)\\n\\nvector_store = PGVectorStore.create_sync(\\n    engine=pg_engine,\\n    table_name=\\'test_table\\',\\n    embedding_service=embeddings\\n)\\nCopypip install -qU langchain-pinecone\\nCopyfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\n\\npc = Pinecone(api_key=...)\\nindex = pc.Index(index_name)\\n\\nvector_store = PineconeVectorStore(embedding=embeddings, index=index)\\nCopypip install -qU langchain-qdrant\\nCopyfrom qdrant_client.models import Distance, VectorParams\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client import QdrantClient\\n\\nclient = QdrantClient(\":memory:\")\\n\\nvector_size = len(embeddings.embed_query(\"sample text\"))\\n\\nif not client.collection_exists(\"test\"):\\n    client.create_collection(\\n        collection_name=\"test\",\\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\\n    )\\nvector_store = QdrantVectorStore(\\n    client=client,\\n    collection_name=\"test\",\\n    embedding=embeddings,\\n)\\n\\n1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if youre comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and wont fit in a models finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\nLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case well use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class post-content, post-title, or post-header are relevant, so well remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\nSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this well split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\nStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n2. Retrieval and generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow lets write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\nRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding arguments for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLets test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directly for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphs Agentic RAG tutorial for more advanced formulations.\\nRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\n Benefits DrawbacksSearch only when needed  The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls  When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries  By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control  The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed  The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLets try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\nNext steps\\nNow that weve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployment\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNextIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/sql-agent', 'title': 'Build a SQL agent - Docs by LangChain', 'language': 'en'}, page_content='Build a SQL agent - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a SQL agentLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsSetupInstallationLangSmith1. Select an LLM2. Configure the database3. Add tools for database interactions4. Use create_agent5. Run the agent(Optional) Use Studio6. Implement human-in-the-loop reviewNext stepsTutorialsLangChainBuild a SQL agentCopy pageCopy pageOverview\\nIn this tutorial, you will learn how to build an agent that can answer questions about a SQL database using LangChain agents.\\nAt a high level, the agent will:\\n1Fetch the available tables and schemas from the database2Decide which tables are relevant to the question3Fetch the schemas for the relevant tables4Generate a query based on the question and information from the schemas5Double-check the query for common mistakes using an LLM6Execute the query and return the results7Correct mistakes surfaced by the database engine until the query is successful8Formulate a response based on the results\\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agents needs. This will mitigate, though not eliminate, the risks of building a model-driven system.\\nConcepts\\nWe will cover the following concepts:\\n\\nTools for reading from SQL databases\\nLangChain agents\\nHuman-in-the-loop processes\\n\\nSetup\\nInstallation\\npipCopypip install langchain  langgraph  langchain-community\\n\\nLangSmith\\nSet up LangSmith to inspect what is happening inside your chain or agent. Then set the following environment variables:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\n1. Select an LLM\\nSelect a model that supports tool-calling:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFace\\uf8ff Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\uf8ff Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\n\\uf8ff Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\uf8ff Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\n\\uf8ff Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\uf8ff Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\nThe output shown in the examples below used OpenAI.\\n2. Configure the database\\nYou will be creating a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the chinook database, which is a sample database that represents a digital media store.\\nFor convenience, we have hosted the database (Chinook.db) on a public GCS bucket.\\nCopyimport requests, pathlib\\n\\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\\nlocal_path = pathlib.Path(\"Chinook.db\")\\n\\nif local_path.exists():\\n    print(f\"{local_path} already exists, skipping download.\")\\nelse:\\n    response = requests.get(url)\\n    if response.status_code == 200:\\n        local_path.write_bytes(response.content)\\n        print(f\"File downloaded and saved as {local_path}\")\\n    else:\\n        print(f\"Failed to download the file. Status code: {response.status_code}\")\\n\\nWe will use a handy SQL database wrapper available in the langchain_community package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\\nCopyfrom langchain_community.utilities import SQLDatabase\\n\\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\\n\\nprint(f\"Dialect: {db.dialect}\")\\nprint(f\"Available tables: {db.get_usable_table_names()}\")\\nprint(f\\'Sample output: {db.run(\"SELECT * FROM Artist LIMIT 5;\")}\\')\\n\\nCopyDialect: sqlite\\nAvailable tables: [\\'Album\\', \\'Artist\\', \\'Customer\\', \\'Employee\\', \\'Genre\\', \\'Invoice\\', \\'InvoiceLine\\', \\'MediaType\\', \\'Playlist\\', \\'PlaylistTrack\\', \\'Track\\']\\nSample output: [(1, \\'AC/DC\\'), (2, \\'Accept\\'), (3, \\'Aerosmith\\'), (4, \\'Alanis Morissette\\'), (5, \\'Alice In Chains\\')]\\n\\n3. Add tools for database interactions\\nUse the SQLDatabase wrapper available in the langchain_community package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\\nCopyfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\\n\\ntoolkit = SQLDatabaseToolkit(db=db, llm=model)\\n\\ntools = toolkit.get_tools()\\n\\nfor tool in tools:\\n    print(f\"{tool.name}: {tool.description}\\\\n\")\\n\\nCopysql_db_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column \\'xxxx\\' in \\'field list\\', use sql_db_schema to query the correct table fields.\\n\\nsql_db_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\\n\\nsql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.\\n\\nsql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\\n\\n4. Use create_agent\\nUse create_agent to build a ReAct agent with minimal code. The agent will interpret the request and generate a SQL command, which the tools will execute. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful.\\nInitialize the agent with a descriptive system prompt to customize its behavior:\\nCopysystem_prompt = \"\"\"\\nYou are an agent designed to interact with a SQL database.\\nGiven an input question, create a syntactically correct {dialect} query to run,\\nthen look at the results of the query and return the answer. Unless the user\\nspecifies a specific number of examples they wish to obtain, always limit your\\nquery to at most {top_k} results.\\n\\nYou can order the results by a relevant column to return the most interesting\\nexamples in the database. Never query for all the columns from a specific table,\\nonly ask for the relevant columns given the question.\\n\\nYou MUST double check your query before executing it. If you get an error while\\nexecuting a query, rewrite the query and try again.\\n\\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\\ndatabase.\\n\\nTo start you should ALWAYS look at the tables in the database to see what you\\ncan query. Do NOT skip this step.\\n\\nThen you should query the schema of the most relevant tables.\\n\"\"\".format(\\n    dialect=db.dialect,\\n    top_k=5,\\n)\\n\\nNow, create an agent with the model, tools, and prompt:\\nCopyfrom langchain.agents import create_agent\\n\\n\\nagent = create_agent(\\n    model,\\n    tools,\\n    system_prompt=system_prompt,\\n)\\n\\n5. Run the agent\\nRun the agent on a sample query and observe its behavior:\\nCopyquestion = \"Which genre on average has the longest tracks?\"\\n\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhich genre on average has the longest tracks?\\n================================== Ai Message ==================================\\nTool Calls:\\n  sql_db_list_tables (call_BQsWg8P65apHc8BTJ1NPDvnM)\\n Call ID: call_BQsWg8P65apHc8BTJ1NPDvnM\\n  Args:\\n================================= Tool Message =================================\\nName: sql_db_list_tables\\n\\nAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\\n================================== Ai Message ==================================\\nTool Calls:\\n  sql_db_schema (call_i89tjKECFSeERbuACYm4w0cU)\\n Call ID: call_i89tjKECFSeERbuACYm4w0cU\\n  Args:\\n    table_names: Track, Genre\\n================================= Tool Message =================================\\nName: sql_db_schema\\n\\n\\nCREATE TABLE \"Genre\" (\\n\\t\"GenreId\" INTEGER NOT NULL,\\n\\t\"Name\" NVARCHAR(120),\\n\\tPRIMARY KEY (\"GenreId\")\\n)\\n\\n/*\\n3 rows from Genre table:\\nGenreId\\tName\\n1\\tRock\\n2\\tJazz\\n3\\tMetal\\n*/\\n\\n\\nCREATE TABLE \"Track\" (\\n\\t\"TrackId\" INTEGER NOT NULL,\\n\\t\"Name\" NVARCHAR(200) NOT NULL,\\n\\t\"AlbumId\" INTEGER,\\n\\t\"MediaTypeId\" INTEGER NOT NULL,\\n\\t\"GenreId\" INTEGER,\\n\\t\"Composer\" NVARCHAR(220),\\n\\t\"Milliseconds\" INTEGER NOT NULL,\\n\\t\"Bytes\" INTEGER,\\n\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\\n\\tPRIMARY KEY (\"TrackId\"),\\n\\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),\\n\\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),\\n\\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\\n)\\n\\n/*\\n3 rows from Track table:\\nTrackId\\tName\\tAlbumId\\tMediaTypeId\\tGenreId\\tComposer\\tMilliseconds\\tBytes\\tUnitPrice\\n1\\tFor Those About To Rock (We Salute You)\\t1\\t1\\t1\\tAngus Young, Malcolm Young, Brian Johnson\\t343719\\t11170334\\t0.99\\n2\\tBalls to the Wall\\t2\\t2\\t1\\tU. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann\\t342562\\t5510424\\t0.99\\n3\\tFast As a Shark\\t3\\t2\\t1\\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\\t230619\\t3990994\\t0.99\\n*/\\n================================== Ai Message ==================================\\nTool Calls:\\n  sql_db_query_checker (call_G64yYm6R6UauiVPCXJZMA49b)\\n Call ID: call_G64yYm6R6UauiVPCXJZMA49b\\n  Args:\\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\\n================================= Tool Message =================================\\nName: sql_db_query_checker\\n\\nSELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\\n================================== Ai Message ==================================\\nTool Calls:\\n  sql_db_query (call_AnO3SrhD0ODJBxh6dHMwvHwZ)\\n Call ID: call_AnO3SrhD0ODJBxh6dHMwvHwZ\\n  Args:\\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\\n================================= Tool Message =================================\\nName: sql_db_query\\n\\n[(\\'Sci Fi & Fantasy\\', 2911783.0384615385), (\\'Science Fiction\\', 2625549.076923077), (\\'Drama\\', 2575283.78125), (\\'TV Shows\\', 2145041.0215053763), (\\'Comedy\\', 1585263.705882353)]\\n================================== Ai Message ==================================\\n\\nOn average, the genre with the longest tracks is \"Sci Fi & Fantasy\" with an average track length of approximately 2,911,783 milliseconds. This is followed by \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\"\\n\\nThe agent correctly wrote a query, checked the query, and ran it to inform its final response.\\nYou can inspect all aspects of the above run, including steps taken, tools invoked, what prompts were seen by the LLM, and more in the LangSmith trace.\\n(Optional) Use Studio\\nStudio provides a client side loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like Tell me the scheme of the database or Show me the invoices for the 5 top customers. You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.\\nRun your agent in StudioIn addition to the previously mentioned packages, you will need to:Copypip install -U langgraph-cli[inmem]>=0.4.0\\nIn directory you will run in, you will need a langgraph.json file with the following contents:Copy{\\n  \"dependencies\": [\".\"],\\n  \"graphs\": {\\n      \"agent\": \"./sql_agent.py:agent\",\\n      \"graph\": \"./sql_agent_langgraph.py:graph\"\\n  },\\n  \"env\": \".env\"\\n}\\nCreate a file sql_agent.py and insert this:Copy#sql_agent.py for studio\\nimport pathlib\\n\\nfrom langchain.agents import create_agent\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\\nfrom langchain_community.utilities import SQLDatabase\\nimport requests\\n\\n\\n# Initialize an LLM\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\n# Get the database, store it locally\\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\\nlocal_path = pathlib.Path(\"Chinook.db\")\\n\\nif local_path.exists():\\n    print(f\"{local_path} already exists, skipping download.\")\\nelse:\\n    response = requests.get(url)\\n    if response.status_code == 200:\\n        local_path.write_bytes(response.content)\\n        print(f\"File downloaded and saved as {local_path}\")\\n    else:\\n        print(f\"Failed to download the file. Status code: {response.status_code}\")\\n\\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\\n\\n# Create the tools\\ntoolkit = SQLDatabaseToolkit(db=db, llm=model)\\n\\ntools = toolkit.get_tools()\\n\\nfor tool in tools:\\n    print(f\"{tool.name}: {tool.description}\\\\n\")\\n\\n# Use create_agent\\nsystem_prompt = \"\"\"\\nYou are an agent designed to interact with a SQL database.\\nGiven an input question, create a syntactically correct {dialect} query to run,\\nthen look at the results of the query and return the answer. Unless the user\\nspecifies a specific number of examples they wish to obtain, always limit your\\nquery to at most {top_k} results.\\n\\nYou can order the results by a relevant column to return the most interesting\\nexamples in the database. Never query for all the columns from a specific table,\\nonly ask for the relevant columns given the question.\\n\\nYou MUST double check your query before executing it. If you get an error while\\nexecuting a query, rewrite the query and try again.\\n\\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\\ndatabase.\\n\\nTo start you should ALWAYS look at the tables in the database to see what you\\ncan query. Do NOT skip this step.\\n\\nThen you should query the schema of the most relevant tables.\\n\"\"\".format(\\n    dialect=db.dialect,\\n    top_k=5,\\n)\\n\\nagent = create_agent(\\n    model,\\n    tools,\\n    system_prompt=system_prompt,\\n)\\n\\n6. Implement human-in-the-loop review\\nIt can be prudent to check the agents SQL queries before they are executed for any unintended actions or inefficiencies.\\nLangChain agents feature support for built-in human-in-the-loop middleware to add oversight to agent tool calls. Lets configure the agent to pause for human review on calling the sql_db_query tool:\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware \\nfrom langgraph.checkpoint.memory import InMemorySaver \\n\\n\\nagent = create_agent(\\n    model,\\n    tools,\\n    system_prompt=system_prompt,\\n    middleware=[ \\n        HumanInTheLoopMiddleware( \\n            interrupt_on={\"sql_db_query\": True}, \\n            description_prefix=\"Tool execution pending approval\", \\n        ), \\n    ], \\n    checkpointer=InMemorySaver(), \\n)\\n\\nWeve added a checkpointer to our agent to allow execution to be paused and resumed. See the human-in-the-loop guide for detalis on this as well as available middleware configurations.\\nOn running the agent, it will now pause for review before executing the sql_db_query tool:\\nCopyquestion = \"Which genre on average has the longest tracks?\"\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}} \\n\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\\n    config, \\n    stream_mode=\"values\",\\n):\\n    if \"__interrupt__\" in step: \\n        print(\"INTERRUPTED:\") \\n        interrupt = step[\"__interrupt__\"][0] \\n        for request in interrupt.value[\"action_requests\"]: \\n            print(request[\"description\"]) \\n    elif \"messages\" in step:\\n        step[\"messages\"][-1].pretty_print()\\n    else:\\n        pass\\n\\nCopy...\\n\\nINTERRUPTED:\\nTool execution pending approval\\n\\nTool: sql_db_query\\nArgs: {\\'query\\': \\'SELECT g.Name AS Genre, AVG(t.Milliseconds) AS AvgTrackLength FROM Track t JOIN Genre g ON t.GenreId = g.GenreId GROUP BY g.Name ORDER BY AvgTrackLength DESC LIMIT 1;\\'}\\n\\nWe can resume execution, in this case accepting the query, using Command:\\nCopyfrom langgraph.types import Command \\n\\nfor step in agent.stream(\\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}), \\n    config,\\n    stream_mode=\"values\",\\n):\\n    if \"messages\" in step:\\n        step[\"messages\"][-1].pretty_print()\\n    elif \"__interrupt__\" in step:\\n        print(\"INTERRUPTED:\")\\n        interrupt = step[\"__interrupt__\"][0]\\n        for request in interrupt.value[\"action_requests\"]:\\n            print(request[\"description\"])\\n    else:\\n        pass\\n\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  sql_db_query (call_7oz86Epg7lYRqi9rQHbZPS1U)\\n Call ID: call_7oz86Epg7lYRqi9rQHbZPS1U\\n  Args:\\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgDuration FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgDuration DESC LIMIT 5;\\n================================= Tool Message =================================\\nName: sql_db_query\\n\\n[(\\'Sci Fi & Fantasy\\', 2911783.0384615385), (\\'Science Fiction\\', 2625549.076923077), (\\'Drama\\', 2575283.78125), (\\'TV Shows\\', 2145041.0215053763), (\\'Comedy\\', 1585263.705882353)]\\n================================== Ai Message ==================================\\n\\nThe genre with the longest average track length is \"Sci Fi & Fantasy\" with an average duration of about 2,911,783 milliseconds, followed by \"Science Fiction\" and \"Drama.\"\\n\\nRefer to the human-in-the-loop guide for details.\\nNext steps\\nFor deeper customization, check out this tutorial for implementing a SQL agent directly using LangGraph primitives.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a RAG agent with LangChainPreviousBuild a voice agent with LangChainNextIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/supervisor', 'title': 'Build a personal assistant with subagents - Docs by LangChain', 'language': 'en'}, page_content='Build a personal assistant with subagents - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationMulti-agentBuild a personal assistant with subagentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainMulti-agentSubagents: Personal assistantHandoffs: Customer supportRouter: Knowledge baseSkills: SQL assistantLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewWhy use a supervisor?ConceptsSetupInstallationLangSmithComponents1. Define tools2. Create specialized sub-agentsCreate a calendar agentCreate an email agent3. Wrap sub-agents as tools4. Create the supervisor agent5. Use the supervisorExample 1: Simple single-domain requestExample 2: Complex multi-domain requestComplete working exampleUnderstanding the architecture6. Add human-in-the-loop review7. Advanced: Control information flowPass additional conversational context to sub-agentsControl what supervisor receives8. Key takeawaysNext stepsTutorialsMulti-agentBuild a personal assistant with subagentsCopy pageCopy pageOverview\\nThe supervisor pattern is a multi-agent architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow.\\nIn this tutorial, youll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities:\\n\\nA calendar agent that handles scheduling, availability checking, and event management.\\nAn email agent that manages communication, drafts messages, and sends notifications.\\n\\nWe will also incorporate human-in-the-loop review to allow users to approve, edit, and reject actions (such as outbound emails) as desired.\\nWhy use a supervisor?\\nMulti-agent architectures allow you to partition tools across workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements).\\nConcepts\\nWe will cover the following concepts:\\n\\nMulti-agent systems\\nHuman-in-the-loop review\\n\\nSetup\\nInstallation\\nThis tutorial requires the langchain package:\\npipcondaCopypip install langchain\\n\\nFor more details, see our Installation guide.\\nLangSmith\\nSet up LangSmith to inspect what is happening inside your agent. Then set the following environment variables:\\nbashpythonCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nComponents\\nWe will need to select a chat model from LangChains suite of integrations:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFace\\uf8ff Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\uf8ff Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\n\\uf8ff Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\uf8ff Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\n\\uf8ff Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\uf8ff Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\n1. Define tools\\nStart by defining the tools that require structured inputs. In real applications, these would call actual APIs (Google Calendar, SendGrid, etc.). For this tutorial, youll use stubs to demonstrate the pattern.\\nCopyfrom langchain.tools import tool\\n\\n@tool\\ndef create_calendar_event(\\n    title: str,\\n    start_time: str,       # ISO format: \"2024-01-15T14:00:00\"\\n    end_time: str,         # ISO format: \"2024-01-15T15:00:00\"\\n    attendees: list[str],  # email addresses\\n    location: str = \"\"\\n) -> str:\\n    \"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\"\\n    # Stub: In practice, this would call Google Calendar API, Outlook API, etc.\\n    return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} attendees\"\\n\\n\\n@tool\\ndef send_email(\\n    to: list[str],  # email addresses\\n    subject: str,\\n    body: str,\\n    cc: list[str] = []\\n) -> str:\\n    \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\\n    # Stub: In practice, this would call SendGrid, Gmail API, etc.\\n    return f\"Email sent to {\\', \\'.join(to)} - Subject: {subject}\"\\n\\n\\n@tool\\ndef get_available_time_slots(\\n    attendees: list[str],\\n    date: str,  # ISO format: \"2024-01-15\"\\n    duration_minutes: int\\n) -> list[str]:\\n    \"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\\n    # Stub: In practice, this would query calendar APIs\\n    return [\"09:00\", \"14:00\", \"16:00\"]\\n\\n2. Create specialized sub-agents\\nNext, well create specialized sub-agents that handle each domain.\\nCreate a calendar agent\\nThe calendar agent understands natural language scheduling requests and translates them into precise API calls. It handles date parsing, availability checking, and event creation.\\nCopyfrom langchain.agents import create_agent\\n\\n\\nCALENDAR_AGENT_PROMPT = (\\n    \"You are a calendar scheduling assistant. \"\\n    \"Parse natural language scheduling requests (e.g., \\'next Tuesday at 2pm\\') \"\\n    \"into proper ISO datetime formats. \"\\n    \"Use get_available_time_slots to check availability when needed. \"\\n    \"Use create_calendar_event to schedule events. \"\\n    \"Always confirm what was scheduled in your final response.\"\\n)\\n\\ncalendar_agent = create_agent(\\n    model,\\n    tools=[create_calendar_event, get_available_time_slots],\\n    system_prompt=CALENDAR_AGENT_PROMPT,\\n)\\n\\nTest the calendar agent to see how it handles natural language scheduling:\\nCopyquery = \"Schedule a team meeting next Tuesday at 2pm for 1 hour\"\\n\\nfor step in calendar_agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\\n):\\n    for update in step.values():\\n        for message in update.get(\"messages\", []):\\n            message.pretty_print()\\n\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  get_available_time_slots (call_EIeoeIi1hE2VmwZSfHStGmXp)\\n Call ID: call_EIeoeIi1hE2VmwZSfHStGmXp\\n  Args:\\n    attendees: []\\n    date: 2024-06-18\\n    duration_minutes: 60\\n================================= Tool Message =================================\\nName: get_available_time_slots\\n\\n[\"09:00\", \"14:00\", \"16:00\"]\\n================================== Ai Message ==================================\\nTool Calls:\\n  create_calendar_event (call_zgx3iJA66Ut0W8S3NpT93kEB)\\n Call ID: call_zgx3iJA66Ut0W8S3NpT93kEB\\n  Args:\\n    title: Team Meeting\\n    start_time: 2024-06-18T14:00:00\\n    end_time: 2024-06-18T15:00:00\\n    attendees: []\\n================================= Tool Message =================================\\nName: create_calendar_event\\n\\nEvent created: Team Meeting from 2024-06-18T14:00:00 to 2024-06-18T15:00:00 with 0 attendees\\n================================== Ai Message ==================================\\n\\nThe team meeting has been scheduled for next Tuesday, June 18th, at 2:00 PM and will last for 1 hour. If you need to add attendees or a location, please let me know!\\n\\nThe agent parses next Tuesday at 2pm into ISO format (2024-01-16T14:00:00), calculates the end time, calls create_calendar_event, and returns a natural language confirmation.\\nCreate an email agent\\nThe email agent handles message composition and sending. It focuses on extracting recipient information, crafting appropriate subject lines and body text, and managing email communication.\\nCopyEMAIL_AGENT_PROMPT = (\\n    \"You are an email assistant. \"\\n    \"Compose professional emails based on natural language requests. \"\\n    \"Extract recipient information and craft appropriate subject lines and body text. \"\\n    \"Use send_email to send the message. \"\\n    \"Always confirm what was sent in your final response.\"\\n)\\n\\nemail_agent = create_agent(\\n    model,\\n    tools=[send_email],\\n    system_prompt=EMAIL_AGENT_PROMPT,\\n)\\n\\nTest the email agent with a natural language request:\\nCopyquery = \"Send the design team a reminder about reviewing the new mockups\"\\n\\nfor step in email_agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\\n):\\n    for update in step.values():\\n        for message in update.get(\"messages\", []):\\n            message.pretty_print()\\n\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  send_email (call_OMl51FziTVY6CRZvzYfjYOZr)\\n Call ID: call_OMl51FziTVY6CRZvzYfjYOZr\\n  Args:\\n    to: [\\'[email\\xa0protected]\\']\\n    subject: Reminder: Please Review the New Mockups\\n    body: Hi Design Team,\\n\\nThis is a friendly reminder to review the new mockups at your earliest convenience. Your feedback is important to ensure that we stay on track with our project timeline.\\n\\nPlease let me know if you have any questions or need additional information.\\n\\nThank you!\\n\\nBest regards,\\n================================= Tool Message =================================\\nName: send_email\\n\\nEmail sent to [email\\xa0protected] - Subject: Reminder: Please Review the New Mockups\\n================================== Ai Message ==================================\\n\\nI\\'ve sent a reminder to the design team asking them to review the new mockups. If you need any further communication on this topic, just let me know!\\n\\nThe agent infers the recipient from the informal request, crafts a professional subject line and body, calls send_email, and returns a confirmation. Each sub-agent has a narrow focus with domain-specific tools and prompts, allowing it to excel at its specific task.\\n3. Wrap sub-agents as tools\\nNow wrap each sub-agent as a tool that the supervisor can invoke. This is the key architectural step that creates the layered system. The supervisor will see high-level tools like schedule_event, not low-level tools like create_calendar_event.\\nCopy@tool\\ndef schedule_event(request: str) -> str:\\n    \"\"\"Schedule calendar events using natural language.\\n\\n    Use this when the user wants to create, modify, or check calendar appointments.\\n    Handles date/time parsing, availability checking, and event creation.\\n\\n    Input: Natural language scheduling request (e.g., \\'meeting with design team\\n    next Tuesday at 2pm\\')\\n    \"\"\"\\n    result = calendar_agent.invoke({\\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\\n    })\\n    return result[\"messages\"][-1].text\\n\\n\\n@tool\\ndef manage_email(request: str) -> str:\\n    \"\"\"Send emails using natural language.\\n\\n    Use this when the user wants to send notifications, reminders, or any email\\n    communication. Handles recipient extraction, subject generation, and email\\n    composition.\\n\\n    Input: Natural language email request (e.g., \\'send them a reminder about\\n    the meeting\\')\\n    \"\"\"\\n    result = email_agent.invoke({\\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\\n    })\\n    return result[\"messages\"][-1].text\\n\\nThe tool descriptions help the supervisor decide when to use each tool, so make them clear and specific. We return only the sub-agents final response, as the supervisor doesnt need to see intermediate reasoning or tool calls.\\n4. Create the supervisor agent\\nNow create the supervisor that orchestrates the sub-agents. The supervisor only sees high-level tools and makes routing decisions at the domain level, not the individual API level.\\nCopySUPERVISOR_PROMPT = (\\n    \"You are a helpful personal assistant. \"\\n    \"You can schedule calendar events and send emails. \"\\n    \"Break down user requests into appropriate tool calls and coordinate the results. \"\\n    \"When a request involves multiple actions, use multiple tools in sequence.\"\\n)\\n\\nsupervisor_agent = create_agent(\\n    model,\\n    tools=[schedule_event, manage_email],\\n    system_prompt=SUPERVISOR_PROMPT,\\n)\\n\\n5. Use the supervisor\\nNow test your complete system with complex requests that require coordination across multiple domains:\\nExample 1: Simple single-domain request\\nCopyquery = \"Schedule a team standup for tomorrow at 9am\"\\n\\nfor step in supervisor_agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\\n):\\n    for update in step.values():\\n        for message in update.get(\"messages\", []):\\n            message.pretty_print()\\n\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  schedule_event (call_mXFJJDU8bKZadNUZPaag8Lct)\\n Call ID: call_mXFJJDU8bKZadNUZPaag8Lct\\n  Args:\\n    request: Schedule a team standup for tomorrow at 9am with Alice and Bob.\\n================================= Tool Message =================================\\nName: schedule_event\\n\\nThe team standup has been scheduled for tomorrow at 9:00 AM with Alice and Bob. If you need to make any changes or add more details, just let me know!\\n================================== Ai Message ==================================\\n\\nThe team standup with Alice and Bob is scheduled for tomorrow at 9:00 AM. If you need any further arrangements or adjustments, please let me know!\\n\\nThe supervisor identifies this as a calendar task, calls schedule_event, and the calendar agent handles date parsing and event creation.\\nFor full transparency into the information flow, including prompts and responses for each chat model call, check out the LangSmith trace for the above run.\\nExample 2: Complex multi-domain request\\nCopyquery = (\\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\\n    \"and send them an email reminder about reviewing the new mockups.\"\\n)\\n\\nfor step in supervisor_agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\\n):\\n    for update in step.values():\\n        for message in update.get(\"messages\", []):\\n            message.pretty_print()\\n\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  schedule_event (call_YA68mqF0koZItCFPx0kGQfZi)\\n Call ID: call_YA68mqF0koZItCFPx0kGQfZi\\n  Args:\\n    request: meeting with the design team next Tuesday at 2pm for 1 hour\\n  manage_email (call_XxqcJBvVIuKuRK794ZIzlLxx)\\n Call ID: call_XxqcJBvVIuKuRK794ZIzlLxx\\n  Args:\\n    request: send the design team an email reminder about reviewing the new mockups\\n================================= Tool Message =================================\\nName: schedule_event\\n\\nYour meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm. Let me know if you need to add more details or make any changes!\\n================================= Tool Message =================================\\nName: manage_email\\n\\nI\\'ve sent an email reminder to the design team requesting them to review the new mockups. If you need to include more information or recipients, just let me know!\\n================================== Ai Message ==================================\\n\\nYour meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm.\\n\\nI\\'ve also sent an email reminder to the design team, asking them to review the new mockups.\\n\\nLet me know if you\\'d like to add more details to the meeting or include additional information in the email!\\n\\nThe supervisor recognizes this requires both calendar and email actions, calls schedule_event for the meeting, then calls manage_email for the reminder. Each sub-agent completes its task, and the supervisor synthesizes both results into a coherent response.\\nRefer to the LangSmith trace to see the detailed information flow for the above run, including individual chat model prompts and responses.\\nComplete working example\\nHeres everything together in a runnable script:\\nShow View complete codeCopy\"\"\"\\nPersonal Assistant Supervisor Example\\n\\nThis example demonstrates the tool calling pattern for multi-agent systems.\\nA supervisor agent coordinates specialized sub-agents (calendar and email)\\nthat are wrapped as tools.\\n\"\"\"\\n\\nfrom langchain.tools import tool\\nfrom langchain.agents import create_agent\\nfrom langchain.chat_models import init_chat_model\\n\\n# ============================================================================\\n# Step 1: Define low-level API tools (stubbed)\\n# ============================================================================\\n\\n@tool\\ndef create_calendar_event(\\n    title: str,\\n    start_time: str,  # ISO format: \"2024-01-15T14:00:00\"\\n    end_time: str,    # ISO format: \"2024-01-15T15:00:00\"\\n    attendees: list[str],  # email addresses\\n    location: str = \"\"\\n) -> str:\\n    \"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\"\\n    return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} attendees\"\\n\\n\\n@tool\\ndef send_email(\\n    to: list[str],      # email addresses\\n    subject: str,\\n    body: str,\\n    cc: list[str] = []\\n) -> str:\\n    \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\\n    return f\"Email sent to {\\', \\'.join(to)} - Subject: {subject}\"\\n\\n\\n@tool\\ndef get_available_time_slots(\\n    attendees: list[str],\\n    date: str,  # ISO format: \"2024-01-15\"\\n    duration_minutes: int\\n) -> list[str]:\\n    \"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\\n    return [\"09:00\", \"14:00\", \"16:00\"]\\n\\n\\n# ============================================================================\\n# Step 2: Create specialized sub-agents\\n# ============================================================================\\n\\nmodel = init_chat_model(\"claude-haiku-4-5-20251001\")  # for example\\n\\ncalendar_agent = create_agent(\\n    model,\\n    tools=[create_calendar_event, get_available_time_slots],\\n    system_prompt=(\\n        \"You are a calendar scheduling assistant. \"\\n        \"Parse natural language scheduling requests (e.g., \\'next Tuesday at 2pm\\') \"\\n        \"into proper ISO datetime formats. \"\\n        \"Use get_available_time_slots to check availability when needed. \"\\n        \"Use create_calendar_event to schedule events. \"\\n        \"Always confirm what was scheduled in your final response.\"\\n    )\\n)\\n\\nemail_agent = create_agent(\\n    model,\\n    tools=[send_email],\\n    system_prompt=(\\n        \"You are an email assistant. \"\\n        \"Compose professional emails based on natural language requests. \"\\n        \"Extract recipient information and craft appropriate subject lines and body text. \"\\n        \"Use send_email to send the message. \"\\n        \"Always confirm what was sent in your final response.\"\\n    )\\n)\\n\\n# ============================================================================\\n# Step 3: Wrap sub-agents as tools for the supervisor\\n# ============================================================================\\n\\n@tool\\ndef schedule_event(request: str) -> str:\\n    \"\"\"Schedule calendar events using natural language.\\n\\n    Use this when the user wants to create, modify, or check calendar appointments.\\n    Handles date/time parsing, availability checking, and event creation.\\n\\n    Input: Natural language scheduling request (e.g., \\'meeting with design team\\n    next Tuesday at 2pm\\')\\n    \"\"\"\\n    result = calendar_agent.invoke({\\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\\n    })\\n    return result[\"messages\"][-1].text\\n\\n\\n@tool\\ndef manage_email(request: str) -> str:\\n    \"\"\"Send emails using natural language.\\n\\n    Use this when the user wants to send notifications, reminders, or any email\\n    communication. Handles recipient extraction, subject generation, and email\\n    composition.\\n\\n    Input: Natural language email request (e.g., \\'send them a reminder about\\n    the meeting\\')\\n    \"\"\"\\n    result = email_agent.invoke({\\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\\n    })\\n    return result[\"messages\"][-1].text\\n\\n\\n# ============================================================================\\n# Step 4: Create the supervisor agent\\n# ============================================================================\\n\\nsupervisor_agent = create_agent(\\n    model,\\n    tools=[schedule_event, manage_email],\\n    system_prompt=(\\n        \"You are a helpful personal assistant. \"\\n        \"You can schedule calendar events and send emails. \"\\n        \"Break down user requests into appropriate tool calls and coordinate the results. \"\\n        \"When a request involves multiple actions, use multiple tools in sequence.\"\\n    )\\n)\\n\\n# ============================================================================\\n# Step 5: Use the supervisor\\n# ============================================================================\\n\\nif __name__ == \"__main__\":\\n    # Example: User request requiring both calendar and email coordination\\n    user_request = (\\n        \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\\n        \"and send them an email reminder about reviewing the new mockups.\"\\n    )\\n\\n    print(\"User Request:\", user_request)\\n    print(\"\\\\n\" + \"=\"*80 + \"\\\\n\")\\n\\n    for step in supervisor_agent.stream(\\n        {\"messages\": [{\"role\": \"user\", \"content\": user_request}]}\\n    ):\\n        for update in step.values():\\n            for message in update.get(\"messages\", []):\\n                message.pretty_print()\\n\\nUnderstanding the architecture\\nYour system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations. The top layer contains the supervisor that routes to high-level capabilities and synthesizes results.\\nThis separation of concerns provides several benefits: each layer has a focused responsibility, you can add new domains without affecting existing ones, and you can test and iterate on each layer independently.\\n6. Add human-in-the-loop review\\nIt can be prudent to incorporate human-in-the-loop review of sensitive actions. LangChain includes built-in middleware to review tool calls, in this case the tools invoked by sub-agents.\\nLets add human-in-the-loop review to both sub-agents:\\n\\nWe configure the create_calendar_event and send_email tools to interrupt, permitting all response types (approve, edit, reject)\\nWe add a checkpointer only to the top-level agent. This is required to pause and resume execution.\\n\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware \\nfrom langgraph.checkpoint.memory import InMemorySaver \\n\\n\\ncalendar_agent = create_agent(\\n    model,\\n    tools=[create_calendar_event, get_available_time_slots],\\n    system_prompt=CALENDAR_AGENT_PROMPT,\\n    middleware=[ \\n        HumanInTheLoopMiddleware( \\n            interrupt_on={\"create_calendar_event\": True}, \\n            description_prefix=\"Calendar event pending approval\", \\n        ), \\n    ], \\n)\\n\\nemail_agent = create_agent(\\n    model,\\n    tools=[send_email],\\n    system_prompt=EMAIL_AGENT_PROMPT,\\n    middleware=[ \\n        HumanInTheLoopMiddleware( \\n            interrupt_on={\"send_email\": True}, \\n            description_prefix=\"Outbound email pending approval\", \\n        ), \\n    ], \\n)\\n\\nsupervisor_agent = create_agent(\\n    model,\\n    tools=[schedule_event, manage_email],\\n    system_prompt=SUPERVISOR_PROMPT,\\n    checkpointer=InMemorySaver(), \\n)\\n\\nLets repeat the query. Note that we gather interrupt events into a list to access downstream:\\nCopyquery = (\\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\\n    \"and send them an email reminder about reviewing the new mockups.\"\\n)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"6\"}}\\n\\ninterrupts = []\\nfor step in supervisor_agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    config,\\n):\\n    for update in step.values():\\n        if isinstance(update, dict):\\n            for message in update.get(\"messages\", []):\\n                message.pretty_print()\\n        else:\\n            interrupt_ = update[0]\\n            interrupts.append(interrupt_)\\n            print(f\"\\\\nINTERRUPTED: {interrupt_.id}\")\\n\\nCopy================================== Ai Message ==================================\\nTool Calls:\\n  schedule_event (call_t4Wyn32ohaShpEZKuzZbl83z)\\n Call ID: call_t4Wyn32ohaShpEZKuzZbl83z\\n  Args:\\n    request: Schedule a meeting with the design team next Tuesday at 2pm for 1 hour.\\n  manage_email (call_JWj4vDJ5VMnvkySymhCBm4IR)\\n Call ID: call_JWj4vDJ5VMnvkySymhCBm4IR\\n  Args:\\n    request: Send an email reminder to the design team about reviewing the new mockups before our meeting next Tuesday at 2pm.\\n\\nINTERRUPTED: 4f994c9721682a292af303ec1a46abb7\\n\\nINTERRUPTED: 2b56f299be313ad8bc689eff02973f16\\n\\nThis time weve interrupted execution. Lets inspect the interrupt events:\\nCopyfor interrupt_ in interrupts:\\n    for request in interrupt_.value[\"action_requests\"]:\\n        print(f\"INTERRUPTED: {interrupt_.id}\")\\n        print(f\"{request[\\'description\\']}\\\\n\")\\n\\nCopyINTERRUPTED: 4f994c9721682a292af303ec1a46abb7\\nCalendar event pending approval\\n\\nTool: create_calendar_event\\nArgs: {\\'title\\': \\'Meeting with the Design Team\\', \\'start_time\\': \\'2024-06-18T14:00:00\\', \\'end_time\\': \\'2024-06-18T15:00:00\\', \\'attendees\\': [\\'design team\\']}\\n\\nINTERRUPTED: 2b56f299be313ad8bc689eff02973f16\\nOutbound email pending approval\\n\\nTool: send_email\\nArgs: {\\'to\\': [\\'[email\\xa0protected]\\'], \\'subject\\': \\'Reminder: Review New Mockups Before Meeting Next Tuesday at 2pm\\', \\'body\\': \"Hello Team,\\\\n\\\\nThis is a reminder to review the new mockups ahead of our meeting scheduled for next Tuesday at 2pm. Your feedback and insights will be valuable for our discussion and next steps.\\\\n\\\\nPlease ensure you\\'ve gone through the designs and are ready to share your thoughts during the meeting.\\\\n\\\\nThank you!\\\\n\\\\nBest regards,\\\\n[Your Name]\"}\\n\\nWe can specify decisions for each interrupt by referring to its ID using a Command. Refer to the human-in-the-loop guide for additional details. For demonstration purposes, here we will accept the calendar event, but edit the subject of the outbound email:\\nCopyfrom langgraph.types import Command \\n\\nresume = {}\\nfor interrupt_ in interrupts:\\n    if interrupt_.id == \"2b56f299be313ad8bc689eff02973f16\":\\n        # Edit email\\n        edited_action = interrupt_.value[\"action_requests\"][0].copy()\\n        edited_action[\"arguments\"][\"subject\"] = \"Mockups reminder\"\\n        resume[interrupt_.id] = {\\n            \"decisions\": [{\"type\": \"edit\", \"edited_action\": edited_action}]\\n        }\\n    else:\\n        resume[interrupt_.id] = {\"decisions\": [{\"type\": \"approve\"}]}\\n\\ninterrupts = []\\nfor step in supervisor_agent.stream(\\n    Command(resume=resume), \\n    config,\\n):\\n    for update in step.values():\\n        if isinstance(update, dict):\\n            for message in update.get(\"messages\", []):\\n                message.pretty_print()\\n        else:\\n            interrupt_ = update[0]\\n            interrupts.append(interrupt_)\\n            print(f\"\\\\nINTERRUPTED: {interrupt_.id}\")\\n\\nCopy================================= Tool Message =================================\\nName: schedule_event\\n\\nYour meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm.\\n================================= Tool Message =================================\\nName: manage_email\\n\\nYour email reminder to the design team has been sent. Heres what was sent:\\n\\n- Recipient: [email\\xa0protected]\\n- Subject: Mockups reminder\\n- Body: A reminder to review the new mockups before the meeting next Tuesday at 2pm, with a request for feedback and readiness for discussion.\\n\\nLet me know if you need any further assistance!\\n================================== Ai Message ==================================\\n\\n- Your meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm.\\n- An email reminder has been sent to the design team about reviewing the new mockups before the meeting.\\n\\nLet me know if you need any further assistance!\\n\\nThe run proceeds with our input.\\n7. Advanced: Control information flow\\nBy default, sub-agents receive only the request string from the supervisor. You might want to pass additional context, such as conversation history or user preferences.\\nPass additional conversational context to sub-agents\\nCopyfrom langchain.tools import tool, ToolRuntime\\n\\n@tool\\ndef schedule_event(\\n    request: str,\\n    runtime: ToolRuntime\\n) -> str:\\n    \"\"\"Schedule calendar events using natural language.\"\"\"\\n    # Customize context received by sub-agent\\n    original_user_message = next(\\n        message for message in runtime.state[\"messages\"]\\n        if message.type == \"human\"\\n    )\\n    prompt = (\\n        \"You are assisting with the following user inquiry:\\\\n\\\\n\"\\n        f\"{original_user_message.text}\\\\n\\\\n\"\\n        \"You are tasked with the following sub-request:\\\\n\\\\n\"\\n        f\"{request}\"\\n    )\\n    result = calendar_agent.invoke({\\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\\n    })\\n    return result[\"messages\"][-1].text\\n\\nThis allows sub-agents to see the full conversation context, which can be useful for resolving ambiguities like schedule it for the same time tomorrow (referencing a previous conversation).\\nYou can see the full context received by the sub agent in the chat model call of the LangSmith trace.\\nControl what supervisor receives\\nYou can also customize what information flows back to the supervisor:\\nCopyimport json\\n\\n@tool\\ndef schedule_event(request: str) -> str:\\n    \"\"\"Schedule calendar events using natural language.\"\"\"\\n    result = calendar_agent.invoke({\\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\\n    })\\n\\n    # Option 1: Return just the confirmation message\\n    return result[\"messages\"][-1].text\\n\\n    # Option 2: Return structured data\\n    # return json.dumps({\\n    #     \"status\": \"success\",\\n    #     \"event_id\": \"evt_123\",\\n    #     \"summary\": result[\"messages\"][-1].text\\n    # })\\n\\nImportant: Make sure sub-agent prompts emphasize that their final message should contain all relevant information. A common failure mode is sub-agents that perform tool calls but dont include the results in their final response.\\n8. Key takeaways\\nThe supervisor pattern creates layers of abstraction where each layer has a clear responsibility. When designing a supervisor system, start with clear domain boundaries and give each sub-agent focused tools and prompts. Write clear tool descriptions for the supervisor, test each layer independently before integration, and control information flow based on your specific needs.\\nWhen to use the supervisor patternUse the supervisor pattern when you have multiple distinct domains (calendar, email, CRM, database), each domain has multiple tools or complex logic, you want centralized workflow control, and sub-agents dont need to converse directly with users.For simpler cases with just a few tools, use a single agent. When agents need to have conversations with users, use handoffs instead. For peer-to-peer collaboration between agents, consider other multi-agent patterns.\\nNext steps\\nLearn about handoffs for agent-to-agent conversations, explore context engineering to fine-tune information flow, read the multi-agent overview to compare different patterns, and use LangSmith to debug and monitor your multi-agent system.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a voice agent with LangChainPreviousBuild customer support with handoffsNextIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/voice-agent', 'title': 'Build a voice agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a voice agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a voice agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewWhat are voice agents?How do voice agents work?1. STT > Agent > TTS architecture (The Sandwich)2. Speech-to-Speech architecture (S2S)Demo Application overviewArchitectureSetup1. Speech-to-textKey conceptsImplementation2. LangChain agentKey conceptsImplementation3. Text-to-speechKey conceptsImplementationPutting it all togetherTutorialsLangChainBuild a voice agent with LangChainCopy pageCopy page\\u200bOverview\\nChat interfaces have dominated how we interact with AI, but recent breakthroughs in multimodal AI are opening up exciting new possibilities. High-quality generative models and expressive text-to-speech (TTS) systems now make it possible to build agents that feel less like tools and more like conversational partners.\\nVoice agents are one example of this. Instead of relying on a keyboard and mouse to type inputs into an agent, you can use spoken words to interact with it. This can be a more natural and engaging way to interact with AI, and can be especially useful for certain contexts.\\n\\u200bWhat are voice agents?\\nVoice agents are agents that can engage in natural spoken conversations with users. These agents combine speech recognition, natural language processing, generative AI, and text-to-speech technologies to create seamless, natural conversations.\\nTheyre suited for a variety of use cases, including:\\n\\nCustomer support\\nPersonal assistants\\nHands-free interfaces\\nCoaching and training\\n\\n\\u200bHow do voice agents work?\\nAt a high level, every voice agent needs to handle three tasks:\\n\\nListen - capture audio and transcribe it\\nThink - interpret intent, reason, plan\\nSpeak - generate audio and stream it back to the user\\n\\nThe difference lies in how these steps are sequenced and coupled. In practice, production agents follow one of two main architectures:\\n\\u200b1. STT > Agent > TTS architecture (The Sandwich)\\nThe Sandwich architecture composes three distinct components: speech-to-text (STT), a text-based LangChain agent, and text-to-speech (TTS).\\n\\nPros:\\n\\nFull control over each component (swap STT/TTS providers as needed)\\nAccess to latest capabilities from modern text-modality models\\nTransparent behavior with clear boundaries between components\\n\\nCons:\\n\\nRequires orchestrating multiple services\\nAdditional complexity in managing the pipeline\\nConversion from speech to text loses information (e.g., tone, emotion)\\n\\n\\u200b2. Speech-to-Speech architecture (S2S)\\nSpeech-to-speech uses a multimodal model that processes audio input and generates audio output natively.\\n\\nPros:\\n\\nSimpler architecture with fewer moving parts\\nTypically lower latency for simple interactions\\nDirect audio processing captures tone and other nuances of speech\\n\\nCons:\\n\\nLimited model options, greater risk of provider lock-in\\nFeatures may lag behind text-modality models\\nLess transparency in how audio is processed\\nReduced controllability and customization options\\n\\nThis guide demonstrates the sandwich architecture to balance performance, controllability, and access to modern model capabilities. The sandwich can achieve sub-700ms latency with some STT and TTS providers while maintaining control over modular components.\\n\\u200bDemo Application overview\\nWell walk through building a voice-based agent using the sandwich architecture. The agent will manage orders for a sandwich shop. The application will demonstrate all three components of the sandwich architecture, using AssemblyAI for STT and Cartesia for TTS (although adapters can be built for most providers).\\nAn end-to-end reference application is available in the voice-sandwich-demo repository. We will walk through that application here.\\nThe demo uses WebSockets for real-time bidirectional communication between the browser and server. The same architecture can be adapted for other transports like telephony systems (Twilio, Vonage) or WebRTC connections.\\n\\u200bArchitecture\\nThe demo implements a streaming pipeline where each stage processes data asynchronously:\\nClient (Browser)\\n\\nCaptures microphone audio and encodes it as PCM\\nEstablishes WebSocket connection to the backend server\\nStreams audio chunks to the server in real-time\\nReceives and plays back synthesized speech audio\\n\\nServer (Python)\\n\\n\\nAccepts WebSocket connections from clients\\n\\n\\nOrchestrates the three-step pipeline:\\n\\nSpeech-to-text (STT): Forwards audio to the STT provider (e.g., AssemblyAI), receives transcript events\\nAgent: Processes transcripts with LangChain agent, streams response tokens\\nText-to-speech (TTS): Sends agent responses to the TTS provider (e.g., Cartesia), receives audio chunks\\n\\n\\n\\nReturns synthesized audio to the client for playback\\n\\n\\nThe pipeline uses async generators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.\\n\\u200bSetup\\nFor detailed installation instructions and setup, see the repository README.\\n\\u200b1. Speech-to-text\\nThe STT stage transforms an incoming audio stream into text transcripts. The implementation uses a producer-consumer pattern to handle audio streaming and transcript reception concurrently.\\n\\u200bKey concepts\\nProducer-Consumer Pattern: Audio chunks are sent to the STT service concurrently with receiving transcript events. This allows transcription to begin before all audio has arrived.\\nEvent Types:\\n\\nstt_chunk: Partial transcripts provided as the STT service processes audio\\nstt_output: Final, formatted transcripts that trigger agent processing\\n\\nWebSocket Connection: Maintains a persistent connection to AssemblyAIs real-time STT API, configured for 16kHz PCM audio with automatic turn formatting.\\n\\u200bImplementation\\nCopyfrom typing import AsyncIterator\\nimport asyncio\\nfrom assemblyai_stt import AssemblyAISTT\\nfrom events import VoiceAgentEvent\\n\\nasync def stt_stream(\\n    audio_stream: AsyncIterator[bytes],\\n) -> AsyncIterator[VoiceAgentEvent]:\\n    \"\"\"\\n    Transform stream: Audio (Bytes)  Voice Events (VoiceAgentEvent)\\n\\n    Uses a producer-consumer pattern where:\\n    - Producer: Reads audio chunks and sends them to AssemblyAI\\n    - Consumer: Receives transcription events from AssemblyAI\\n    \"\"\"\\n    stt = AssemblyAISTT(sample_rate=16000)\\n\\n    async def send_audio():\\n        \"\"\"Background task that pumps audio chunks to AssemblyAI.\"\"\"\\n        try:\\n            async for audio_chunk in audio_stream:\\n                await stt.send_audio(audio_chunk)\\n        finally:\\n            # Signal completion when audio stream ends\\n            await stt.close()\\n\\n    # Launch audio sending in background\\n    send_task = asyncio.create_task(send_audio())\\n\\n    try:\\n        # Receive and yield transcription events as they arrive\\n        async for event in stt.receive_events():\\n            yield event\\n    finally:\\n        # Cleanup\\n        with contextlib.suppress(asyncio.CancelledError):\\n            send_task.cancel()\\n            await send_task\\n        await stt.close()\\n\\nThe application implements an AssemblyAI client to manage the WebSocket connection and message parsing. See below for implementations; similar adapters can be constructed for other STT providers.\\nAssemblyAI ClientCopyclass AssemblyAISTT:\\n    def __init__(self, api_key: str | None = None, sample_rate: int = 16000):\\n        self.api_key = api_key or os.getenv(\"ASSEMBLYAI_API_KEY\")\\n        self.sample_rate = sample_rate\\n        self._ws: WebSocketClientProtocol | None = None\\n\\n    async def send_audio(self, audio_chunk: bytes) -> None:\\n        \"\"\"Send PCM audio bytes to AssemblyAI.\"\"\"\\n        ws = await self._ensure_connection()\\n        await ws.send(audio_chunk)\\n\\n    async def receive_events(self) -> AsyncIterator[STTEvent]:\\n        \"\"\"Yield STT events as they arrive from AssemblyAI.\"\"\"\\n        async for raw_message in self._ws:\\n            message = json.loads(raw_message)\\n\\n            if message[\"type\"] == \"Turn\":\\n                # Final formatted transcript\\n                if message.get(\"turn_is_formatted\"):\\n                    yield STTOutputEvent.create(message[\"transcript\"])\\n                # Partial transcript\\n                else:\\n                    yield STTChunkEvent.create(message[\"transcript\"])\\n\\n    async def _ensure_connection(self) -> WebSocketClientProtocol:\\n        \"\"\"Establish WebSocket connection if not already connected.\"\"\"\\n        if self._ws is None:\\n            url = f\"wss://streaming.assemblyai.com/v3/ws?sample_rate={self.sample_rate}&format_turns=true\"\\n            self._ws = await websockets.connect(\\n                url,\\n                additional_headers={\"Authorization\": self.api_key}\\n            )\\n        return self._ws\\n\\n\\u200b2. LangChain agent\\nThe agent stage processes text transcripts through a LangChain agent and streams the response tokens. In this case, we stream all text content blocks generated by the agent.\\n\\u200bKey concepts\\nStreaming Responses: The agent uses stream_mode=\"messages\" to emit response tokens as theyre generated, rather than waiting for the complete response. This enables the TTS stage to begin synthesis immediately.\\nConversation Memory: A checkpointer maintains conversation state across turns using a unique thread ID. This allows the agent to reference previous exchanges in the conversation.\\n\\u200bImplementation\\nCopyfrom uuid import uuid4\\nfrom langchain.agents import create_agent\\nfrom langchain.messages import HumanMessage\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n\\n# Define agent tools\\ndef add_to_order(item: str, quantity: int) -> str:\\n    \"\"\"Add an item to the customer\\'s sandwich order.\"\"\"\\n    return f\"Added {quantity} x {item} to the order.\"\\n\\ndef confirm_order(order_summary: str) -> str:\\n    \"\"\"Confirm the final order with the customer.\"\"\"\\n    return f\"Order confirmed: {order_summary}. Sending to kitchen.\"\\n\\n# Create agent with tools and memory\\nagent = create_agent(\\n    model=\"anthropic:claude-haiku-4-5\",  # Select your model\\n    tools=[add_to_order, confirm_order],\\n    system_prompt=\"\"\"You are a helpful sandwich shop assistant.\\n    Your goal is to take the user\\'s order. Be concise and friendly.\\n    Do NOT use emojis, special characters, or markdown.\\n    Your responses will be read by a text-to-speech engine.\"\"\",\\n    checkpointer=InMemorySaver(),\\n)\\n\\nasync def agent_stream(\\n    event_stream: AsyncIterator[VoiceAgentEvent],\\n) -> AsyncIterator[VoiceAgentEvent]:\\n    \"\"\"\\n    Transform stream: Voice Events  Voice Events (with Agent Responses)\\n\\n    Passes through all upstream events and adds agent_chunk events\\n    when processing STT transcripts.\\n    \"\"\"\\n    # Generate unique thread ID for conversation memory\\n    thread_id = str(uuid4())\\n\\n    async for event in event_stream:\\n        # Pass through all upstream events\\n        yield event\\n\\n        # Process final transcripts through the agent\\n        if event.type == \"stt_output\":\\n            # Stream agent response with conversation context\\n            stream = agent.astream(\\n                {\"messages\": [HumanMessage(content=event.transcript)]},\\n                {\"configurable\": {\"thread_id\": thread_id}},\\n                stream_mode=\"messages\",\\n            )\\n\\n            # Yield agent response chunks as they arrive\\n            async for message, _ in stream:\\n                if message.text:\\n                    yield AgentChunkEvent.create(message.text)\\n\\n\\u200b3. Text-to-speech\\nThe TTS stage synthesizes agent response text into audio and streams it back to the client. Like the STT stage, it uses a producer-consumer pattern to handle concurrent text sending and audio reception.\\n\\u200bKey concepts\\nConcurrent Processing: The implementation merges two async streams:\\n\\nUpstream processing: Passes through all events and sends agent text chunks to the TTS provider\\nAudio reception: Receives synthesized audio chunks from the TTS provider\\n\\nStreaming TTS: Some providers (such as Cartesia) begin synthesizing audio as soon as it receives text, enabling audio playback to start before the agent finishes generating its complete response.\\nEvent Passthrough: All upstream events flow through unchanged, allowing the client or other observers to track the full pipeline state.\\n\\u200bImplementation\\nCopyfrom cartesia_tts import CartesiaTTS\\nfrom utils import merge_async_iters\\n\\nasync def tts_stream(\\n    event_stream: AsyncIterator[VoiceAgentEvent],\\n) -> AsyncIterator[VoiceAgentEvent]:\\n    \"\"\"\\n    Transform stream: Voice Events  Voice Events (with Audio)\\n\\n    Merges two concurrent streams:\\n    1. process_upstream(): passes through events and sends text to Cartesia\\n    2. tts.receive_events(): yields audio chunks from Cartesia\\n    \"\"\"\\n    tts = CartesiaTTS()\\n\\n    async def process_upstream() -> AsyncIterator[VoiceAgentEvent]:\\n        \"\"\"Process upstream events and send agent text to Cartesia.\"\"\"\\n        async for event in event_stream:\\n            # Pass through all events\\n            yield event\\n            # Send agent text to Cartesia for synthesis\\n            if event.type == \"agent_chunk\":\\n                await tts.send_text(event.text)\\n\\n    try:\\n        # Merge upstream events with TTS audio events\\n        # Both streams run concurrently\\n        async for event in merge_async_iters(\\n            process_upstream(),\\n            tts.receive_events()\\n        ):\\n            yield event\\n    finally:\\n        await tts.close()\\n\\nThe application implements an Cartesia client to manage the WebSocket connection and audio streaming. See below for implementations; similar adapters can be constructed for other TTS providers.\\nCartesia ClientCopyimport base64\\nimport json\\nimport websockets\\n\\nclass CartesiaTTS:\\n    def __init__(\\n        self,\\n        api_key: Optional[str] = None,\\n        voice_id: str = \"f6ff7c0c-e396-40a9-a70b-f7607edb6937\",\\n        model_id: str = \"sonic-3\",\\n        sample_rate: int = 24000,\\n        encoding: str = \"pcm_s16le\",\\n    ):\\n        self.api_key = api_key or os.getenv(\"CARTESIA_API_KEY\")\\n        self.voice_id = voice_id\\n        self.model_id = model_id\\n        self.sample_rate = sample_rate\\n        self.encoding = encoding\\n        self._ws: WebSocketClientProtocol | None = None\\n\\n    def _generate_context_id(self) -> str:\\n        \"\"\"Generate a valid context_id for Cartesia.\"\"\"\\n        timestamp = int(time.time() * 1000)\\n        counter = self._context_counter\\n        self._context_counter += 1\\n        return f\"ctx_{timestamp}_{counter}\"\\n\\n    async def send_text(self, text: str | None) -> None:\\n        \"\"\"Send text to Cartesia for synthesis.\"\"\"\\n        if not text or not text.strip():\\n            return\\n\\n        ws = await self._ensure_connection()\\n        payload = {\\n            \"model_id\": self.model_id,\\n            \"transcript\": text,\\n            \"voice\": {\\n                \"mode\": \"id\",\\n                \"id\": self.voice_id,\\n            },\\n            \"output_format\": {\\n                \"container\": \"raw\",\\n                \"encoding\": self.encoding,\\n                \"sample_rate\": self.sample_rate,\\n            },\\n            \"language\": self.language,\\n            \"context_id\": self._generate_context_id(),\\n        }\\n        await ws.send(json.dumps(payload))\\n\\n    async def receive_events(self) -> AsyncIterator[TTSChunkEvent]:\\n        \"\"\"Yield audio chunks as they arrive from Cartesia.\"\"\"\\n        async for raw_message in self._ws:\\n            message = json.loads(raw_message)\\n\\n            # Decode and yield audio chunks\\n            if \"data\" in message and message[\"data\"]:\\n                audio_chunk = base64.b64decode(message[\"data\"])\\n                if audio_chunk:\\n                    yield TTSChunkEvent.create(audio_chunk)\\n\\n    async def _ensure_connection(self) -> WebSocketClientProtocol:\\n        \"\"\"Establish WebSocket connection if not already connected.\"\"\"\\n        if self._ws is None:\\n            url = (\\n                f\"wss://api.cartesia.ai/tts/websocket\"\\n                f\"?api_key={self.api_key}&cartesia_version={self.cartesia_version}\"\\n            )\\n            self._ws = await websockets.connect(url)\\n\\n        return self._ws\\n\\n\\u200bPutting it all together\\nThe complete pipeline chains the three stages together:\\nCopyfrom langchain_core.runnables import RunnableGenerator\\n\\npipeline = (\\n    RunnableGenerator(stt_stream)      # Audio  STT events\\n    | RunnableGenerator(agent_stream)  # STT events  Agent events\\n    | RunnableGenerator(tts_stream)    # Agent events  TTS audio\\n)\\n\\n# Use in WebSocket endpoint\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n    await websocket.accept()\\n\\n    async def websocket_audio_stream():\\n        \"\"\"Yield audio bytes from WebSocket.\"\"\"\\n        while True:\\n            data = await websocket.receive_bytes()\\n            yield data\\n\\n    # Transform audio through pipeline\\n    output_stream = pipeline.atransform(websocket_audio_stream())\\n\\n    # Send TTS audio back to client\\n    async for event in output_stream:\\n        if event.type == \"tts_chunk\":\\n            await websocket.send_bytes(event.audio)\\n\\nWe use RunnableGenerators to compose each step of the pipeline. This is an abstraction LangChain uses internally to manage streaming across components.\\nEach stage processes events independently and concurrently: audio transcription begins as soon as audio arrives, the agent starts reasoning as soon as a transcript is available, and speech synthesis begins as soon as agent text is generated. This architecture can achieve sub-700ms latency to support natural conversation.\\nFor more on building agents with LangChain, see the Agents guide.\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a SQL agentPreviousBuild a personal assistant with subagentsNextIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "docs_list=[item for sublist in docs for item in sublist]\n",
    "docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "37e358bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi! How are you today? Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 11, 'total_tokens': 34, 'completion_time': 0.06014975, 'prompt_time': 4.9089e-05, 'queue_time': 0.068052535, 'total_time': 0.060198839}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_ba95244fa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--80bd7a3b-4e51-40c0-84e5-eb0a72be5fc7-0', usage_metadata={'input_tokens': 11, 'output_tokens': 23, 'total_tokens': 34})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000,chunk_overlap=200)\n",
    "embeddings=HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "llm=ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e71c5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_splits=text_splitter.split_documents(docs_list)\n",
    "\n",
    "vector_store=FAISS.from_documents(doc_splits,embeddings)\n",
    "\n",
    "retriever=vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75fa3372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='cc2131db-a688-4be1-b507-c992f0fedb7c', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/sql-agent', 'title': 'Build a SQL agent - Docs by LangChain', 'language': 'en'}, page_content='Build a SQL agent - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a SQL agentLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsSetupInstallationLangSmith1. Select an LLM2. Configure the database3. Add tools for database interactions4. Use create_agent5. Run the agent(Optional) Use Studio6. Implement human-in-the-loop reviewNext stepsTutorialsLangChainBuild a SQL agentCopy pageCopy pageOverview\\nIn this tutorial, you will learn how to build an agent that can answer questions about a SQL database using LangChain agents.\\nAt a high level, the agent will:\\n1Fetch the available tables and schemas from the database2Decide which tables are relevant to the question3Fetch the schemas for the relevant tables4Generate a query based on the question and information from the schemas5Double-check the query for common mistakes using an LLM6Execute the query and return the results7Correct mistakes surfaced by the database engine until the query is successful8Formulate a response based on the results\\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agents needs. This will mitigate, though not eliminate, the risks of building a model-driven system.\\nConcepts\\nWe will cover the following concepts:\\n\\nTools for reading from SQL databases\\nLangChain agents\\nHuman-in-the-loop processes\\n\\nSetup\\nInstallation\\npipCopypip install langchain  langgraph  langchain-community\\n\\nLangSmith\\nSet up LangSmith to inspect what is happening inside your chain or agent. Then set the following environment variables:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\n1. Select an LLM\\nSelect a model that supports tool-calling:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFace\\uf8ff Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\uf8ff Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\n\\uf8ff Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\uf8ff Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model'),\n",
       " Document(id='14a609d0-ad3c-4e87-a6ca-312186d6627d', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/sql-agent', 'title': 'Build a SQL agent - Docs by LangChain', 'language': 'en'}, page_content='for tool in tools:\\n    print(f\"{tool.name}: {tool.description}\\\\n\")\\n\\n# Use create_agent\\nsystem_prompt = \"\"\"\\nYou are an agent designed to interact with a SQL database.\\nGiven an input question, create a syntactically correct {dialect} query to run,\\nthen look at the results of the query and return the answer. Unless the user\\nspecifies a specific number of examples they wish to obtain, always limit your\\nquery to at most {top_k} results.\\n\\nYou can order the results by a relevant column to return the most interesting\\nexamples in the database. Never query for all the columns from a specific table,\\nonly ask for the relevant columns given the question.\\n\\nYou MUST double check your query before executing it. If you get an error while\\nexecuting a query, rewrite the query and try again.\\n\\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\\ndatabase.\\n\\nTo start you should ALWAYS look at the tables in the database to see what you\\ncan query. Do NOT skip this step.\\n\\nThen you should query the schema of the most relevant tables.\\n\"\"\".format(\\n    dialect=db.dialect,\\n    top_k=5,\\n)\\n\\nagent = create_agent(\\n    model,\\n    tools,\\n    system_prompt=system_prompt,\\n)\\n\\n6. Implement human-in-the-loop review\\nIt can be prudent to check the agents SQL queries before they are executed for any unintended actions or inefficiencies.\\nLangChain agents feature support for built-in human-in-the-loop middleware to add oversight to agent tool calls. Lets configure the agent to pause for human review on calling the sql_db_query tool:\\nCopyfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware \\nfrom langgraph.checkpoint.memory import InMemorySaver \\n\\n\\nagent = create_agent(\\n    model,\\n    tools,\\n    system_prompt=system_prompt,\\n    middleware=[ \\n        HumanInTheLoopMiddleware( \\n            interrupt_on={\"sql_db_query\": True}, \\n            description_prefix=\"Tool execution pending approval\", \\n        ), \\n    ], \\n    checkpointer=InMemorySaver(), \\n)\\n\\nWeve added a checkpointer to our agent to allow execution to be paused and resumed. See the human-in-the-loop guide for detalis on this as well as available middleware configurations.\\nOn running the agent, it will now pause for review before executing the sql_db_query tool:\\nCopyquestion = \"Which genre on average has the longest tracks?\"\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}} \\n\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\\n    config, \\n    stream_mode=\"values\",\\n):\\n    if \"__interrupt__\" in step: \\n        print(\"INTERRUPTED:\") \\n        interrupt = step[\"__interrupt__\"][0] \\n        for request in interrupt.value[\"action_requests\"]: \\n            print(request[\"description\"]) \\n    elif \"messages\" in step:\\n        step[\"messages\"][-1].pretty_print()\\n    else:\\n        pass\\n\\nCopy...\\n\\nINTERRUPTED:\\nTool execution pending approval\\n\\nTool: sql_db_query\\nArgs: {\\'query\\': \\'SELECT g.Name AS Genre, AVG(t.Milliseconds) AS AvgTrackLength FROM Track t JOIN Genre g ON t.GenreId = g.GenreId GROUP BY g.Name ORDER BY AvgTrackLength DESC LIMIT 1;\\'}'),\n",
       " Document(id='3c1fff64-e298-41d4-913c-b08d63b1ea1d', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/supervisor', 'title': 'Build a personal assistant with subagents - Docs by LangChain', 'language': 'en'}, page_content='Build a personal assistant with subagents - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationMulti-agentBuild a personal assistant with subagentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainMulti-agentSubagents: Personal assistantHandoffs: Customer supportRouter: Knowledge baseSkills: SQL assistantLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewWhy use a supervisor?ConceptsSetupInstallationLangSmithComponents1. Define tools2. Create specialized sub-agentsCreate a calendar agentCreate an email agent3. Wrap sub-agents as tools4. Create the supervisor agent5. Use the supervisorExample 1: Simple single-domain requestExample 2: Complex multi-domain requestComplete working exampleUnderstanding the architecture6. Add human-in-the-loop review7. Advanced: Control information flowPass additional conversational context to sub-agentsControl what supervisor receives8. Key takeawaysNext stepsTutorialsMulti-agentBuild a personal assistant with subagentsCopy pageCopy pageOverview\\nThe supervisor pattern is a multi-agent architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow.\\nIn this tutorial, youll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities:\\n\\nA calendar agent that handles scheduling, availability checking, and event management.\\nAn email agent that manages communication, drafts messages, and sends notifications.\\n\\nWe will also incorporate human-in-the-loop review to allow users to approve, edit, and reject actions (such as outbound emails) as desired.\\nWhy use a supervisor?\\nMulti-agent architectures allow you to partition tools across workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements).\\nConcepts\\nWe will cover the following concepts:\\n\\nMulti-agent systems\\nHuman-in-the-loop review\\n\\nSetup\\nInstallation\\nThis tutorial requires the langchain package:\\npipcondaCopypip install langchain\\n\\nFor more details, see our Installation guide.\\nLangSmith\\nSet up LangSmith to inspect what is happening inside your agent. Then set the following environment variables:\\nbashpythonCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nComponents\\nWe will need to select a chat model from LangChains suite of integrations:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFace\\uf8ff Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\uf8ff Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\n\\uf8ff Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model'),\n",
       " Document(id='ef306258-6c9a-4da1-a873-72e76c6e23e4', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/rag', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and generationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy pageOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\nConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce weve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or youre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\nPreview\\nIn this guide well build an app that answers questions about the websites content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what is sql agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3208af7",
   "metadata": {},
   "source": [
    "#### GRAPH STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa1acd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_classic.schema import Document\n",
    "from typing_extensions import TypedDict\n",
    "class GraphState(TypedDict):\n",
    "    \" Represents the state of the graph\"\n",
    "\n",
    "    question:str\n",
    "    generation:str\n",
    "    web_search:str\n",
    "    documents:List[Document]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1fd93f",
   "metadata": {},
   "source": [
    "#### RETRIEVE NODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0dedcb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"Retrieve documents from the vector store by using retriever\"\n",
    "\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question=state[\"question\"]\n",
    "\n",
    "    documents=retriever.invoke(question)\n",
    "\n",
    "\n",
    "    return {\"documents\":documents,\"question\":question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7318cb6",
   "metadata": {},
   "source": [
    "#### DOCUMENTS GRADING NODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed51f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "def grade(state):\n",
    "    \"Checks whether the retrieved documents are relevant to the question\"\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO THE QUERY---\")\n",
    "\n",
    "    class GradeDocuments(BaseModel):\n",
    "        \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(\n",
    "            description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "        )\n",
    "\n",
    "    llm_grader=llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "    \n",
    "    # Prompt\n",
    "    system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "    retrieval_grader=grade_prompt|llm_grader\n",
    "\n",
    "    question=state[\"question\"]\n",
    "    documents=state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "    grade = score.binary_score\n",
    "    if grade == \"yes\":\n",
    "        print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "        filtered_docs.append(d)\n",
    "    else:\n",
    "        print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "        web_search = \"Yes\"\n",
    "        \n",
    "\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "559ba4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"rlm/rag-prompt\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb26b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "rag_chain=prompt|llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e7184",
   "metadata": {},
   "source": [
    "#### GENERATE NODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "23e85648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \" generate answer \"\n",
    "\n",
    "    print(\"---Generate---\")\n",
    "    question=state[\"question\"]\n",
    "    documents=state[\"documents\"]\n",
    "\n",
    "\n",
    "    # Convert Document objects  strings\n",
    "    context = \"\\n\\n\".join(\n",
    "        d.page_content if hasattr(d, \"page_content\") else str(d)\n",
    "        for d in documents\n",
    "    )\n",
    "\n",
    "    generation=rag_chain.invoke({\"context\":context,\"question\":question})\n",
    "    \n",
    "    return {\"documents\":documents,\"question\":question,\"generation\":generation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2a0c5",
   "metadata": {},
   "source": [
    "#### RE-WRITE QUERY NODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3bec3532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(state):\n",
    "    \"Transform the query to produce a better question\"\n",
    "\n",
    "    print(\"---REWRITE QUERY---\")\n",
    "\n",
    "    # Prompt\n",
    "    system = \"\"\"You are a query rewriting agent.\n",
    "\n",
    "        Rewrite the user's question to a SHORT, SEARCH-OPTIMIZED QUESTION.\n",
    "\n",
    "        RULES:\n",
    "        - Return ONLY the rewritten question.\n",
    "        - DO NOT add explanations.\n",
    "        - DO NOT say \"based on the original question\".\n",
    "        - DO NOT create paragraphs.\n",
    "        - NO formatting.\n",
    "        - NO markdown.\n",
    "        - ONLY return the rewritten question, nothing else.\n",
    "        \"\"\"\n",
    "    re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "    question=state[\"question\"]\n",
    "    documents=state[\"documents\"]\n",
    "\n",
    "    new_question=question_rewriter.invoke({\"question\":question})\n",
    "\n",
    "    print(new_question)\n",
    "\n",
    "    return {\"documents\":documents,\"question\":new_question}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6cbb7",
   "metadata": {},
   "source": [
    "#### WEB SEARCH NODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e465acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.schema import Document\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question=state[\"question\"]\n",
    "    documents=state[\"documents\"]\n",
    "\n",
    "    docs=web_search_tool.invoke({\"query\":question})\n",
    "    normalized_docs = []\n",
    "\n",
    "    # Case 1: list of dicts\n",
    "    if isinstance(docs, list):\n",
    "        for d in docs:\n",
    "            if isinstance(d, dict) and \"content\" in d:\n",
    "                normalized_docs.append(Document(page_content=d[\"content\"]))\n",
    "            elif isinstance(d, Document):\n",
    "                normalized_docs.append(d)\n",
    "\n",
    "    # Case 2: single string\n",
    "    elif isinstance(docs, str):\n",
    "        normalized_docs.append(Document(page_content=docs))\n",
    "\n",
    "    # Case 3: anything else\n",
    "    else:\n",
    "        normalized_docs.append(Document(page_content=str(docs)))\n",
    "\n",
    "    return {\"documents\": normalized_docs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308b417c",
   "metadata": {},
   "source": [
    "#### ROUTING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5129e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_generate(state):\n",
    "    \"Decides whether to generate an answer or re-generate a question\"\n",
    "\n",
    "    print(\"---DECIDE TO GENERATE OR RE-WRITE QUERY---\")\n",
    "    state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
    "        return \"rewrite_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de89aa8",
   "metadata": {},
   "source": [
    "#### GRAPH STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "17ea205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "builder=StateGraph(GraphState)\n",
    "\n",
    "builder.add_node(\"retrieve\",retrieve)\n",
    "builder.add_node(\"grade\",grade)\n",
    "builder.add_node(\"generate\",generate)\n",
    "builder.add_node(\"rewrite_query\",rewrite_query)\n",
    "builder.add_node(\"web_search\",web_search)\n",
    "\n",
    "builder.add_edge(START,\"retrieve\")\n",
    "builder.add_edge(\"retrieve\",\"grade\")\n",
    "builder.add_conditional_edges(\"grade\",decide_generate,{\"rewrite_query\":\"rewrite_query\",\n",
    "                                                       \"generate\":\"generate\"})\n",
    "\n",
    "builder.add_edge(\"rewrite_query\",\"web_search\")\n",
    "\n",
    "builder.add_edge(\"web_search\",\"generate\")\n",
    "\n",
    "builder.add_edge(\"generate\",END)\n",
    "\n",
    "graph=builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "26aa5712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display,Image\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "398261ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO THE QUERY---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---DECIDE TO GENERATE OR RE-WRITE QUERY---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "---REWRITE QUERY---\n",
      "What is a supervisor agent?\n",
      "---WEB SEARCH---\n",
      "---Generate---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is a supervisor agent?',\n",
       " 'generation': AIMessage(content='A supervisor agent is a central agent that coordinates and manages the work of specialized sub-agents or worker agents, delegating tasks and responsibilities to them. It collects input, maintains structured memory, and orchestrates the execution of sub-agents based on predefined logic. The supervisor agent serves as a hub that routes questions, breaks down user requests, and ensures efficient and structured execution of complex tasks.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 1789, 'total_tokens': 1868, 'completion_time': 0.203709412, 'prompt_time': 0.050110566, 'queue_time': 0.105801238, 'total_time': 0.253819978}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_ba95244fa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--e6ed0702-254c-4044-b31f-35a1273be512-0', usage_metadata={'input_tokens': 1789, 'output_tokens': 79, 'total_tokens': 1868}),\n",
       " 'web_search': 'Yes',\n",
       " 'documents': [Document(metadata={}, page_content='The supervisor agent uses the instructions you provide to understand the structure and role of each collaborator agent. To ensure that the team performs well, you must clearly designate the role and responsibilities of the supervisor agent and every collaborator agent on the team and minimize overlapping responsibilities. You can describe each agents role and responsibilities using natural language. For example, you could use multi-agent collaboration to create an online mortgage assistant. Each Amazon Bedrock agent can be configured to carry out one of the following tasks:\\n\\n   Supervisor agent  Takes questions from the user, checks if the question is about the existing mortgage, new mortgage, or it is a general question and routes the question to the appropriate collaborator agent. [...] With multi-agent approach, you can quickly designate an Amazon Bedrock Agent as the supervisor and then associate one or more collaborator agents with the supervisor. You can use this hierarchical collaboration model to synchronously respond to prompts and queries from users in real-time. As your hierarchical model matures, you can add additional collaborator agents to augment its capabilities.\\n\\n###### Important\\n\\nBefore you can enable multi-agent collaboration, you must first save the surpervisor agent. After the supervisor agent has been saved, you can update the agent and associate additional collaborator agents to it.'),\n",
       "  Document(metadata={}, page_content='# Build a personal assistant with subagents\\n\\n## \\u200b Overview\\n\\nThe supervisor pattern is a multi-agent architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow. In this tutorial, youll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities: [...] \"\"\" \"\"\" Personal Assistant Supervisor Example Personal Assistant Supervisor Example This example demonstrates the tool calling pattern for multi-agent systems.This example demonstrates the tool calling pattern for multi-agent systems.A supervisor agent coordinates specialized sub-agents (calendar and email)A supervisor agent coordinates specialized sub-agents (calendar and email)that are wrapped as tools.that are wrapped as tools. \"\"\" \"\"\" from langchain.tools import tool from langchain.tools import  toolfrom langchain.agents import create_agent from langchain.agents import  create_agentfrom langchain.chat_models import init_chat_model from langchain.chat_models import  init_chat_model # ============================================================================# [...] Step 4: Create the supervisor agent# Step 4: Create the supervisor agent# ============================================================================# ============================================================================ supervisor_agent = create_agent(supervisor_agent = create_agent( model, model, tools=[schedule_event, manage_email],  tools =[schedule_event, manage_email], system_prompt=( system_prompt =( \"You are a helpful personal assistant. \" \"You are a helpful personal assistant. \" \"You can schedule calendar events and send emails. \" \"You can schedule calendar events and send emails. \" \"Break down user requests into appropriate tool calls and coordinate the results. \" \"Break down user requests into appropriate tool calls and coordinate the results. \" \"When a request involves'),\n",
       "  Document(metadata={}, page_content='### Creating a Supervisor Agent via Agent Dashboard\\n\\n1. Navigate to the Agents Page - Click \"New Agent.\"\\n2. Select Supervisor Agent - Choose it from the available agent types.\\n3. Configure Agent Settings - Adjust memory, iteration limits, and agent paths.\\n4. Deploy the Agent - Save and activate it in your workflow.\\n\\n## Configuration Options [...] ## Agent Path Configuration\\n\\nThe Supervisor Agent allows you to define different execution paths for specialized sub-agents. Each agent path represents a distinct functionality or responsibility within your workflow.\\n\\n### Configuring Agent Paths\\n\\nFor each agent path, you\\'ll need to define three key components:\\n\\n1. Agent Name: A descriptive identifier for the sub-agent\\'s role (e.g., \"Researcher\", \"Writer\", \"Editor\")\\n2. Agent Description: A clear explanation of the sub-agent\\'s purpose and responsibilities\\n3. Agent Schema: A structured JSON schema defining the expected output format from the agent\\n\\n### Defining Agent Schema\\n\\nThe agent schema uses a JSON structure to specify the format and requirements for the agent\\'s output. In the example shown: [...] Docs\\n\\nAgents\\n\\nSupervisor Agent\\n\\n# Supervisor Agent\\n\\nThe Supervisor Agent is a specialized AI agent in Lamatic.ai designed to manage and coordinate multi-agent flow. It serves as the central hub, collecting input, maintaining structured memory, and orchestrating the execution of sub-agents based on predefined logic. This ensures an efficient, iterative process for dynamic AI-powered applications such as structured decision-making systems and task automation.\\n\\n## Why Use the Supervisor Agent?\\n\\n Orchestrate Complex AI Flow: Manage multiple agents dynamically, ensuring structured execution.\\n Maintain Context and Memory: Retain past interactions to improve continuity in multi-step tasks.\\n Enable Adaptive Execution: Define agent paths, loop conditions, and AI-generated decision trees.'),\n",
       "  Document(metadata={}, page_content='A supervisoragent hierarchy is an organizational and computational architecture in which control, decision-making, and information processing responsibilities are distributed across multiple levels. In such hierarchies, higher-level entitiesreferred to as supervisorsdelegate sub-tasks or roles to subordinate agents. This decomposition enables efficient management of complexity, improved modularity, and enhanced scalability, while also posing unique challenges around information consistency, coordination, and robustness. The following sections synthesize the key principles, mathematical formulations, empirical findings, and architectural mechanisms underpinning supervisoragent hierarchies across diverse domains.\\n\\n## 1. Formal Structure and Task Decomposition [...] ## 1. Formal Structure and Task Decomposition\\n\\nSupervisoragent hierarchies most commonly arise from hierarchical task decomposition, where complex activities are recursively split into subtasks (Laird et al., 2011). A supervisor assigns responsibility for a subtask or information aggregation to a lower-level agent or computational unit. Each subagent typically maintains its own local state, reasoning processes, and persistent assumptions, while still operating under constraints propagated from its supervisor. [...] 2000 character limit reached\\n\\n# Supervisor-Agent Hierarchies\\n\\nUpdated 25 August 2025\\n\\n Supervisoragent hierarchies are organizational and computational frameworks that decompose complex tasks into modular subtasks to improve efficiency and scalability.\\n They utilize mechanisms like Dynamic Hierarchical Justification (DHJ) to maintain logical consistency and adapt to contextual changes in real time.\\n Robust designs incorporate error propagation models and decentralized control to optimize information aggregation and ensure reliable supervisory oversight.'),\n",
       "  Document(metadata={}, page_content='After a worker agent completes its assigned task, it returns the control back to the supervisor agent\\n\\n5. Once the supervisor determines that the overall task is complete and all necessary information has been gathered and processed by the relevant worker agents, it generates the final output.\\n\\nLets Build It: A Quick Example with LangGraph\\n\\nReady to see this in action? Heres a Python example of how to create a simple multi-agent system. Well create two worker agents: a research\\\\_agent to search the web for information and a code\\\\_agent to execute Python code for tasks like data visualization. A supervisor will oversee their work, delegating tasks to the right agent.\\n\\nStep 1: Install the Required Libraries\\n\\n```\\npip install langgraph-supervisor langchain-openai\\n``` [...] 3. Delegation and Handoff Tools\\n\\nA crucial aspect of supervisor systems is the ability to transfer control and information between agents. This is typically implemented using handoff tools that allow the supervisor to specify a destination agent and a payload (information) to pass to that agent. When the supervisor calls these tools, it hands off control to a worker agent, passing the full message history to that agent.\\n\\n4. Worker Agents\\n\\nThese agents are designed to perform specific tasks and are equipped with relevant tools. For example:\\n\\n Research Agent: Accesses a web search tool (e.g., Tavily API) to gather information.\\n A Coder Agent: Can execute Python code to do things like perform calculations or create charts. [...] Heres an architectural diagram, explained in detail, illustrating the typical flow within such a system:\\n\\nThe Workflow: From Request to Result\\n\\n1. The process begins with a users request or an initial input that needs to be processed by the multi-agent system.\\n2. Supervisor Agent\\n\\nThis is the central orchestrator of the system. Upon receiving the user query, the supervisor agents role is to understand the task requirements and decide which specialized worker agent to invoke. It uses a Large Language Model (LLM) and a defined prompt to make these delegation decisions. The supervisor does not perform the core tasks itself; it delegates.\\n\\n3. Delegation and Handoff Tools')]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"question\":\"What supervisor agent?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670732e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
